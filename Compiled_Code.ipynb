{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'OA_Candidate_Sampler' from 'c:\\\\Users\\\\bparaju\\\\OneDrive - Oklahoma A and M System\\\\ML Project\\\\Bibek\\\\Project Code\\\\OA_Candidate_Sampler.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Backbone\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from position_encoder import PositionEncoder\n",
    "from Encoder import Encoder\n",
    "import OA_Candidate_Sampler \n",
    "import torch.nn as nn\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from importlib import reload\n",
    "import json\n",
    "\n",
    "reload(Backbone)\n",
    "reload(OA_Candidate_Sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_OA_Pairs.json','r') as f:\n",
    "    UniqeOAPairs = json.load(f)\n",
    "\n",
    "UniqeOAPairs = [tuple(pair) for pair in UniqeOAPairs]\n",
    "num_oas = len(UniqeOAPairs) \n",
    "\n",
    "with open('OA_Pairs.json','r') as f:\n",
    "    id2OAPairs = json.load(f)\n",
    "\n",
    "id2OAPairs = {id: list(set(tuple(pair) for pair in id2OAPairs[id])) for id in id2OAPairs.keys()}\n",
    "\n",
    "with open('vcoco_processed_data_all_2014.json', 'r') as f:\n",
    "    vcoco_data = json.load(f)\n",
    "    \n",
    "ids = list(vcoco_data.keys())\n",
    "\n",
    "train_ids = ids[:8000]\n",
    "test_ids = ids[8000:10000]\n",
    "\n",
    "train_data = [vcoco_data[id] for id in train_ids]\n",
    "test_data = [vcoco_data[id] for id in test_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "id2OAOneHot = {}\n",
    "\n",
    "for id, pairs in id2OAPairs.items():\n",
    "    # Filter pairs that are in UniqeOAPairs\n",
    "    valid_pairs = [pair for pair in pairs if pair in UniqeOAPairs]\n",
    "\n",
    "    # Get indices of valid pairs\n",
    "    indices = [UniqeOAPairs.index(pair) for pair in valid_pairs]\n",
    "\n",
    "    # Convert indices to one-hot encoding\n",
    "    oneHot = nn.functional.one_hot(torch.tensor(indices, dtype=int), num_classes=len(UniqeOAPairs))#gives batch of hot encoding (len(indices), len(UNiqeOAPairs))\n",
    "    \n",
    "    oneHot = oneHot.sum(axis = 0) #converts to single vector (1, len(UNiqeOAPairs))\n",
    "    # Store the one-hot encodings for the ID\n",
    "    id2OAOneHot[id] = oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<PIL.Image.Image image mode=RGB size=500x375>],\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]], dtype=int64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_image(file,url = False):\n",
    "    if(url):\n",
    "      response = requests.get(img_url)\n",
    "      img = Image.open(BytesIO(response.content))\n",
    "      img = img.convert(\"RGB\") #remove alpha channel\n",
    "    else:\n",
    "        folder = '../images/'\n",
    "        with open(folder+file, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            img = img.convert('RGB')\n",
    "    return img\n",
    "def get_batch(size=1):\n",
    "    images = []\n",
    "    y = []\n",
    "    idx = np.random.randint(0,len(train_data)-size)\n",
    "    for i,image in enumerate(train_data[idx:idx+size]):\n",
    "        img_url = image['url']\n",
    "        # img = get_image(img_url, url=True)\n",
    "        img = get_image(image['file_name'])\n",
    "        images.append(img)\n",
    "        try:\n",
    "            output = id2OAOneHot[str(image['id'])]\n",
    "        # print(image['id'],output)\n",
    "        except:\n",
    "            output = torch.zeros(len(UniqeOAPairs)) #if OA pair doesn't exist\n",
    "        y.append(output)\n",
    "        \n",
    "    return images,np.array(y)\n",
    "\n",
    "get_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (list, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image]!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image]!, !Parameter!, !NoneType!, !tuple of (int, int)!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m ims, y \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m feature_map \u001b[38;5;241m=\u001b[39m \u001b[43mBackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#output from backbone\u001b[39;00m\n\u001b[0;32m      5\u001b[0m position_encoded_feature_map \u001b[38;5;241m=\u001b[39m PositionEncoder()(feature_map)\n\u001b[0;32m      6\u001b[0m B,C,W,H \u001b[38;5;241m=\u001b[39m position_encoded_feature_map\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# (Batch, Channels, Width, Height)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\OneDrive - Oklahoma A and M System\\ML Project\\Bibek\\Project Code\\Backbone.py:43\u001b[0m, in \u001b[0;36mBackbone.forward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[0;32m     35\u001b[0m     \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# transformed_images = torch.stack(transformed_images, dim=0) # (batch, 3, 224, 224)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#(batch, 2048, 7, 7)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     reshaped_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_conv(output) \u001b[38;5;66;03m# (batch, reshape_dim,7,7)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reshaped_output\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (list, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image]!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image, Image]!, !Parameter!, !NoneType!, !tuple of (int, int)!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ims, y = get_batch(20)\n",
    "\n",
    "feature_map = Backbone.Backbone()(ims) #output from backbone\n",
    "position_encoded_feature_map = PositionEncoder()(feature_map)\n",
    "B,C,W,H = position_encoded_feature_map.shape # (Batch, Channels, Width, Height)\n",
    "backbone_output_vectorized = position_encoded_feature_map.reshape((B,C,W*H)) #changing 2d image to vector (B, C, F) (F = feature)\n",
    "backbone_output_tansformed = backbone_output_vectorized.permute([0,2,1]) # (B, F, C)\n",
    "# backbone_output_tansformed = position_encoded_feature_map.permute([0,3,2,1]) # (B, H,W, C)\n",
    "encoder_output = Encoder()(backbone_output_tansformed) # (B, F, C)\n",
    "\n",
    "OASampler = OA_Candidate_Sampler.OACandidateSampler(num_oas)\n",
    "probabilities = OASampler(encoder_output)\n",
    "\n",
    "print(probabilities.shape)\n",
    "# confidences, indices = torch.topk(probabilities, 5, dim=-1)\n",
    "# print(\"Top-K OA Candidates (indices):\", indices)\n",
    "# print(\"Top-K OA Candidates (confidences):\", confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class VectorizeFeatureMap(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VectorizeFeatureMap, self).__init__()\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p._require_grad_(False)\n",
    "    def forward(self, feature_map):\n",
    "        B,C,W,H = feature_map.shape # (Batch, Channels, Width, Height)\n",
    "        feature_map = feature_map.reshape((B,C,W*H)) #changing 2d image to vector (B, C, F) (F = feature)\n",
    "        feature_map = feature_map.permute([0,2,1]) # (B, F, C)\n",
    "        \n",
    "        return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HOI_Decoder, HOI_Prediction, SpatialFeatureGenerator, Semantic_Feature_Generator, Feature_Aggregator, Query_Refiner, HOI_Decoder, HOI_Prediction\n",
    "\n",
    "reload(Query_Refiner)\n",
    "reload(HOI_Prediction)\n",
    "backbone = Backbone.Backbone()\n",
    "positionEncoder = PositionEncoder()\n",
    "matrix2Vector = VectorizeFeatureMap()\n",
    "encoder = Encoder()\n",
    "OASampler = OA_Candidate_Sampler.OACandidateSampler(num_oas)\n",
    "spatial_feature_generator = SpatialFeatureGenerator.SpatialFeatureGenerator([])\n",
    "semantic_generator = Semantic_Feature_Generator.SemanticFeatureGenerator()\n",
    "aggregator = Feature_Aggregator.FeatureAggregator()\n",
    "query_refiner = Query_Refiner.QueryRefiner(batch_size)\n",
    "decoder = HOI_Decoder.HOIDecoder()\n",
    "hoi_predictor = HOI_Prediction.HOIPrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "imgs, y = get_batch(batch_size)\n",
    "\n",
    "# Step 1: Initialize and call Backbone\n",
    "features = Backbone.Backbone()(imgs)\n",
    "\n",
    "# Step 2: Pass Backbone output to PositionEncoder\n",
    "position_encoded = PositionEncoder()(features)\n",
    "\n",
    "# Step 3: Pass PositionEncoder output to VectorizeFeatureMap\n",
    "vectorized_features = VectorizeFeatureMap()(position_encoded)\n",
    "\n",
    "# Step 4: Pass VectorizeFeatureMap output to Encoder\n",
    "encoded_features = Encoder()(vectorized_features)\n",
    "\n",
    "# Step 5: Pass Encoder output to OA_Candidate_Sampler\n",
    "oas = OA_Candidate_Sampler.OACandidateSampler(num_oas)(encoded_features)\n",
    "\n",
    "# Step 6: Pass OA_Candidate_Sampler output to SpatialFeatureGenerator\n",
    "spatial_features = SpatialFeatureGenerator.SpatialFeatureGenerator([])(oas)\n",
    "\n",
    "# Step 7: Pass SpatialFeatureGenerator output to SemanticFeatureGenerator\n",
    "semantic_features = Semantic_Feature_Generator.SemanticFeatureGenerator()(oas)\n",
    "\n",
    "# Step 8: Pass both SpatialFeatureGenerator and SemanticFeatureGenerator outputs to FeatureAggregator\n",
    "aggregated_features = Feature_Aggregator.FeatureAggregator()(spatial_features, semantic_features)\n",
    "\n",
    "# Step 9: Pass FeatureAggregator output to QueryRefiner\n",
    "refined_query = Query_Refiner.QueryRefiner(batch_size)(aggregated_features)\n",
    "\n",
    "# Step 10: Pass QueryRefiner output to HOI_Decoder\n",
    "decoded_query = HOI_Decoder.HOIDecoder()(encoded_features, refined_query)\n",
    "\n",
    "# Step 11: Pass HOI_Decoder output to HOI_Prediction\n",
    "bh, bo, po, pa = HOI_Prediction.HOIPrediction()(decoded_query)\n",
    "\n",
    "# Final Output\n",
    "# print(\"Human-Object Interaction Predictions:\", hoi_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pred_obj_logits'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SSRT.__init__() missing 2 required positional arguments: 'nobj' and 'nact'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mSSRT\u001b[39;00m\n\u001b[0;32m      3\u001b[0m reload(SSRT)\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSSRT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSSRT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_oas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m imgs, label \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m model(imgs)\n",
      "\u001b[1;31mTypeError\u001b[0m: SSRT.__init__() missing 2 required positional arguments: 'nobj' and 'nact'"
     ]
    }
   ],
   "source": [
    "import SSRT\n",
    "\n",
    "reload(SSRT)\n",
    "model = SSRT.SSRT(num_oas, batch_size=1)\n",
    "\n",
    "imgs, label = get_batch(1)\n",
    "model(imgs)\n",
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Datasets\n",
    "\n",
    "reload(Datasets)\n",
    "\n",
    "dataset = Datasets.VCOCODataset(img_set=\"train\", img_folder=\"../images/\",anno_file=\"annotations/trainval_vcoco.json\" , num_queries=49, transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    return tuple(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn= collate_fn)\n",
    "\n",
    "# for i, (image, target) in enumerate(loader):\n",
    "#     if(None in image):\n",
    "#         print(image)\n",
    "#     # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Set_Criterion\n",
    "import Matcher\n",
    "reload(Set_Criterion)\n",
    "losses = ['obj_labels', 'verb_labels', 'sub_obj_boxes', 'obj_cardinality']\n",
    "\n",
    "# weight_dict = model.state_dict()\n",
    "\n",
    "weight_dict = {}\n",
    "weight_dict['loss_obj_ce'] = 1\n",
    "weight_dict['loss_verb_ce'] = 1\n",
    "weight_dict['loss_sub_bbox'] = 5\n",
    "weight_dict['loss_obj_bbox'] = 5\n",
    "weight_dict['loss_sub_giou'] = 2\n",
    "weight_dict['loss_obj_giou'] = 2\n",
    "\n",
    "criterion= Set_Criterion.SetCriterionHOI(num_obj_classes=20, \n",
    "                                         num_queries=49, \n",
    "                                         num_verb_classes=29, \n",
    "                                         matcher=Matcher.HungarianMatcherHOI(), \n",
    "                                         weight_dict=weight_dict, \n",
    "                                         eos_coef= 0.1 , \n",
    "                                         losses= losses, verb_loss_type= 'focal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bparaju\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SSRT.__init__() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mSSRT\u001b[39;00m\n\u001b[0;32m     11\u001b[0m reload(SSRT)\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSSRT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSSRT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_oas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m91\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m29\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# images = images.to(device)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28mlist\u001b[39m(images))\n",
      "\u001b[1;31mTypeError\u001b[0m: SSRT.__init__() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "# model = None\n",
    "# def train_one_epoch():\n",
    "#     model.train()\n",
    "#     criterion.train()\n",
    "\n",
    "batch_size = 10\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn= collate_fn)\n",
    "\n",
    "import SSRT\n",
    "\n",
    "reload(SSRT)\n",
    "model = SSRT.SSRT(num_oas, batch_size=batch_size, nobj = 91, nact =29)\n",
    "\n",
    "\n",
    "for images, targets in loader:\n",
    "    # images = images.to(device)\n",
    "    # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    outputs = model(list(images))\n",
    "    loss_dict = criterion(outputs, targets)\n",
    "    weight_dict = criterion.weight_dict\n",
    "    losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "    # reduce losses over all GPUs for logging purposes\n",
    "    # loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "    loss_dict_reduced = loss_dict\n",
    "    loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                    for k, v in loss_dict_reduced.items()}\n",
    "    loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "    losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "    # loss_value = losses_reduced_scaled.item()\n",
    "    losses.backward()\n",
    "\n",
    "    # return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, y = get_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:465\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m         )\n\u001b[1;32m--> 465\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    467\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:80\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected type <class 'list'>"
     ]
    }
   ],
   "source": [
    "transforms.Resize(256)(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Copyright (c) Hitachi, Ltd. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n",
    "# ------------------------------------------------------------------------\n",
    "# Modified from DETR (https://github.com/facebookresearch/detr)\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "# ------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "Train and eval functions used in main.py\n",
    "\"\"\"\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from typing import Iterable\n",
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "import misc \n",
    "\n",
    "reload(misc)\n",
    "\n",
    "\n",
    "utils = misc\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(256), # Resize to 256x256\n",
    "                transforms.CenterCrop(224), # Crop to 224x224\n",
    "                transforms.ToTensor(), # Convert to tensor\n",
    "                transforms.Normalize(  # Normalize according to ImageNet statistics\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "def train_one_epoch(model, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    if hasattr(criterion, 'loss_labels'):\n",
    "        metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    else:\n",
    "        metric_logger.add_meter('obj_class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 1\n",
    "\n",
    "    \n",
    "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        # samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # targets = torch.stack(targets, dim = 0).to(device)\n",
    "        # samples = list(samples)\n",
    "        \n",
    "        transformed_images = []\n",
    "        for image in samples:\n",
    "            transformed_image = transform(image).to(device)\n",
    "            transformed_images.append(transformed_image)\n",
    "\n",
    "\n",
    "        transformed_images = torch.stack(transformed_images, dim=0) # (batch, 3, 224, 224)\n",
    "\n",
    "        transformed_images.to(device)\n",
    "\n",
    "        # Apply transformation to each image and convert it to tensor\n",
    "        # samples = [transform(image) for image in samples]\n",
    "        # samples = torch.stack(samples, dim=0)\n",
    "\n",
    "        outputs = model(transformed_images)\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "        loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if max_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "        if hasattr(criterion, 'loss_labels'):\n",
    "            metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "        else:\n",
    "            metric_logger.update(obj_class_error=loss_dict_reduced['obj_class_error'])\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [  0/540]  eta: 0:47:21  lr: 0.001000  obj_class_error: 100.00  loss: 10960.4668 (10960.4668)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 59.5508 (59.5508)  loss_sub_bbox: 5519.5044 (5519.5044)  loss_obj_bbox: 5372.1865 (5372.1865)  loss_sub_giou: 2.1423 (2.1423)  loss_obj_giou: 2.5704 (2.5704)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 59.5508 (59.5508)  loss_sub_bbox_unscaled: 1103.9009 (1103.9009)  loss_obj_bbox_unscaled: 1074.4373 (1074.4373)  loss_sub_giou_unscaled: 1.0711 (1.0711)  loss_obj_giou_unscaled: 1.2852 (1.2852)  obj_cardinality_error_unscaled: 47.0000 (47.0000)  time: 5.2622  data: 2.0751  max mem: 2345\n",
      "Epoch: [1]  [  1/540]  eta: 0:35:41  lr: 0.001000  obj_class_error: 100.00  loss: 10755.3770 (10857.9219)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 59.5508 (66.5560)  loss_sub_bbox: 5357.7241 (5438.6143)  loss_obj_bbox: 5314.9878 (5343.5872)  loss_sub_giou: 2.1114 (2.1268)  loss_obj_giou: 2.4804 (2.5254)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 59.5508 (66.5560)  loss_sub_bbox_unscaled: 1071.5448 (1087.7228)  loss_obj_bbox_unscaled: 1062.9976 (1068.7174)  loss_sub_giou_unscaled: 1.0557 (1.0634)  loss_obj_giou_unscaled: 1.2402 (1.2627)  obj_cardinality_error_unscaled: 46.8000 (46.9000)  time: 3.9733  data: 1.9476  max mem: 2876\n",
      "Epoch: [1]  [  2/540]  eta: 0:32:03  lr: 0.001000  obj_class_error: 100.00  loss: 10960.4668 (10893.1071)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 59.5508 (56.6298)  loss_sub_bbox: 5519.5044 (5508.8149)  loss_obj_bbox: 5314.9878 (5318.3081)  loss_sub_giou: 2.1423 (2.2377)  loss_obj_giou: 2.5704 (2.6046)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 59.5508 (56.6298)  loss_sub_bbox_unscaled: 1103.9009 (1101.7630)  loss_obj_bbox_unscaled: 1062.9976 (1063.6616)  loss_sub_giou_unscaled: 1.0711 (1.1189)  loss_obj_giou_unscaled: 1.2852 (1.3023)  obj_cardinality_error_unscaled: 46.8000 (46.5667)  time: 3.5749  data: 1.9377  max mem: 2876\n",
      "Epoch: [1]  [  3/540]  eta: 0:30:11  lr: 0.001000  obj_class_error: 100.00  loss: 10960.4668 (11109.0750)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 36.7772 (51.1555)  loss_sub_bbox: 5519.5044 (5582.5280)  loss_obj_bbox: 5314.9878 (5466.0558)  loss_sub_giou: 2.1423 (2.2366)  loss_obj_giou: 2.5342 (2.5870)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 36.7772 (51.1555)  loss_sub_bbox_unscaled: 1103.9009 (1116.5056)  loss_obj_bbox_unscaled: 1062.9976 (1093.2112)  loss_sub_giou_unscaled: 1.0711 (1.1183)  loss_obj_giou_unscaled: 1.2671 (1.2935)  obj_cardinality_error_unscaled: 45.9000 (45.9750)  time: 3.3738  data: 1.9214  max mem: 2876\n",
      "Epoch: [1]  [  4/540]  eta: 0:29:10  lr: 0.001000  obj_class_error: 100.00  loss: 10963.4775 (11082.9211)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 59.5508 (57.0602)  loss_sub_bbox: 5519.5044 (5490.9122)  loss_obj_bbox: 5372.1865 (5525.6440)  loss_sub_giou: 2.2321 (2.2357)  loss_obj_giou: 2.5342 (2.5570)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 59.5508 (57.0602)  loss_sub_bbox_unscaled: 1103.9009 (1098.1824)  loss_obj_bbox_unscaled: 1074.4373 (1105.1288)  loss_sub_giou_unscaled: 1.1161 (1.1179)  loss_obj_giou_unscaled: 1.2671 (1.2785)  obj_cardinality_error_unscaled: 46.8000 (46.1400)  time: 3.2658  data: 1.9390  max mem: 2876\n",
      "Epoch: [1]  [  5/540]  eta: 0:28:06  lr: 0.001000  obj_class_error: 95.83  loss: 10960.4668 (11057.2142)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 59.5508 (57.9727)  loss_sub_bbox: 5357.7241 (5462.5240)  loss_obj_bbox: 5372.1865 (5527.4490)  loss_sub_giou: 2.1423 (2.2130)  loss_obj_giou: 2.4804 (2.5435)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.3056)  loss_verb_ce_unscaled: 59.5508 (57.9727)  loss_sub_bbox_unscaled: 1071.5448 (1092.5048)  loss_obj_bbox_unscaled: 1074.4373 (1105.4898)  loss_sub_giou_unscaled: 1.0711 (1.1065)  loss_obj_giou_unscaled: 1.2402 (1.2718)  obj_cardinality_error_unscaled: 46.6000 (46.2167)  time: 3.1526  data: 1.9080  max mem: 2876\n",
      "Epoch: [1]  [  6/540]  eta: 0:27:18  lr: 0.001000  obj_class_error: 100.00  loss: 10963.4775 (11264.3926)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 59.5508 (57.9994)  loss_sub_bbox: 5519.5044 (5513.8239)  loss_obj_bbox: 5536.4736 (5683.3322)  loss_sub_giou: 2.1423 (2.1979)  loss_obj_giou: 2.4804 (2.5270)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.4048)  loss_verb_ce_unscaled: 59.5508 (57.9994)  loss_sub_bbox_unscaled: 1103.9009 (1102.7648)  loss_obj_bbox_unscaled: 1107.2947 (1136.6664)  loss_sub_giou_unscaled: 1.0711 (1.0989)  loss_obj_giou_unscaled: 1.2402 (1.2635)  obj_cardinality_error_unscaled: 46.7000 (46.2857)  time: 3.0675  data: 1.8829  max mem: 2876\n",
      "Epoch: [1]  [  7/540]  eta: 0:26:54  lr: 0.001000  obj_class_error: 100.00  loss: 10960.4668 (11062.2791)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 58.1599 (55.2807)  loss_sub_bbox: 5357.7241 (5417.1823)  loss_obj_bbox: 5372.1865 (5580.5651)  loss_sub_giou: 2.1423 (2.1963)  loss_obj_giou: 2.4804 (2.5424)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.4792)  loss_verb_ce_unscaled: 58.1599 (55.2807)  loss_sub_bbox_unscaled: 1071.5448 (1083.4365)  loss_obj_bbox_unscaled: 1074.4373 (1116.1130)  loss_sub_giou_unscaled: 1.0711 (1.0982)  loss_obj_giou_unscaled: 1.2402 (1.2712)  obj_cardinality_error_unscaled: 46.6000 (46.1500)  time: 3.0285  data: 1.8875  max mem: 2876\n",
      "Epoch: [1]  [  8/540]  eta: 0:26:53  lr: 0.001000  obj_class_error: 100.00  loss: 10963.4775 (11060.1877)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 58.1657 (55.6012)  loss_sub_bbox: 5519.5044 (5441.1738)  loss_obj_bbox: 5372.1865 (5554.1595)  loss_sub_giou: 2.1502 (2.1912)  loss_obj_giou: 2.5342 (2.5497)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.5370)  loss_verb_ce_unscaled: 58.1657 (55.6012)  loss_sub_bbox_unscaled: 1103.9009 (1088.2348)  loss_obj_bbox_unscaled: 1074.4373 (1110.8319)  loss_sub_giou_unscaled: 1.0751 (1.0956)  loss_obj_giou_unscaled: 1.2671 (1.2749)  obj_cardinality_error_unscaled: 46.6000 (46.1889)  time: 3.0330  data: 1.9277  max mem: 2876\n",
      "Epoch: [1]  [  9/540]  eta: 0:26:36  lr: 0.001000  obj_class_error: 100.00  loss: 10963.4775 (11157.5097)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 58.1599 (53.9485)  loss_sub_bbox: 5519.5044 (5490.7053)  loss_obj_bbox: 5372.1865 (5603.5814)  loss_sub_giou: 2.1502 (2.1996)  loss_obj_giou: 2.5342 (2.5627)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.5833)  loss_verb_ce_unscaled: 58.1599 (53.9485)  loss_sub_bbox_unscaled: 1103.9009 (1098.1411)  loss_obj_bbox_unscaled: 1074.4373 (1120.7163)  loss_sub_giou_unscaled: 1.0751 (1.0998)  loss_obj_giou_unscaled: 1.2671 (1.2814)  obj_cardinality_error_unscaled: 46.5000 (46.0100)  time: 3.0063  data: 1.9299  max mem: 2876\n",
      "Epoch: [1]  [ 10/540]  eta: 0:26:21  lr: 0.001000  obj_class_error: 100.00  loss: 10978.3057 (11208.3816)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 58.1599 (53.3342)  loss_sub_bbox: 5519.5044 (5489.3218)  loss_obj_bbox: 5536.4736 (5656.4333)  loss_sub_giou: 2.1610 (2.1961)  loss_obj_giou: 2.5704 (2.5840)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.6212)  loss_verb_ce_unscaled: 58.1599 (53.3342)  loss_sub_bbox_unscaled: 1103.9009 (1097.8644)  loss_obj_bbox_unscaled: 1107.2947 (1131.2866)  loss_sub_giou_unscaled: 1.0805 (1.0980)  loss_obj_giou_unscaled: 1.2852 (1.2920)  obj_cardinality_error_unscaled: 46.5000 (46.0273)  time: 2.9845  data: 1.9287  max mem: 2876\n",
      "Epoch: [1]  [ 11/540]  eta: 0:26:16  lr: 0.001000  obj_class_error: 100.00  loss: 10963.4775 (11178.4049)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 58.1599 (55.0209)  loss_sub_bbox: 5475.4873 (5468.2083)  loss_obj_bbox: 5529.8921 (5645.8882)  loss_sub_giou: 2.1502 (2.1880)  loss_obj_giou: 2.5704 (2.5872)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.6528)  loss_verb_ce_unscaled: 58.1599 (55.0209)  loss_sub_bbox_unscaled: 1095.0974 (1093.6417)  loss_obj_bbox_unscaled: 1105.9784 (1129.1776)  loss_sub_giou_unscaled: 1.0751 (1.0940)  loss_obj_giou_unscaled: 1.2852 (1.2936)  obj_cardinality_error_unscaled: 46.5000 (46.0917)  time: 2.9801  data: 1.9419  max mem: 2876\n",
      "Epoch: [1]  [ 12/540]  eta: 0:26:04  lr: 0.001000  obj_class_error: 100.00  loss: 10978.3057 (11186.8612)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 58.1599 (54.6368)  loss_sub_bbox: 5475.4873 (5455.2862)  loss_obj_bbox: 5536.4736 (5667.6567)  loss_sub_giou: 2.1606 (2.1858)  loss_obj_giou: 2.5704 (2.5833)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.6795)  loss_verb_ce_unscaled: 58.1599 (54.6368)  loss_sub_bbox_unscaled: 1095.0974 (1091.0572)  loss_obj_bbox_unscaled: 1107.2947 (1133.5313)  loss_sub_giou_unscaled: 1.0803 (1.0929)  loss_obj_giou_unscaled: 1.2852 (1.2917)  obj_cardinality_error_unscaled: 46.6000 (46.1308)  time: 2.9625  data: 1.9388  max mem: 2876\n",
      "Epoch: [1]  [ 13/540]  eta: 0:25:51  lr: 0.001000  obj_class_error: 100.00  loss: 10978.3057 (11252.6313)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (52.9956)  loss_sub_bbox: 5475.4873 (5483.2345)  loss_obj_bbox: 5536.4736 (5707.1032)  loss_sub_giou: 2.1606 (2.1984)  loss_obj_giou: 2.5704 (2.5873)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7024)  loss_verb_ce_unscaled: 50.0278 (52.9956)  loss_sub_bbox_unscaled: 1095.0974 (1096.6469)  loss_obj_bbox_unscaled: 1107.2947 (1141.4206)  loss_sub_giou_unscaled: 1.0803 (1.0992)  loss_obj_giou_unscaled: 1.2852 (1.2937)  obj_cardinality_error_unscaled: 46.5000 (45.9857)  time: 2.9449  data: 1.9336  max mem: 2876\n",
      "Epoch: [1]  [ 14/540]  eta: 0:25:52  lr: 0.001000  obj_class_error: 100.00  loss: 10978.3057 (11206.6663)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (52.7317)  loss_sub_bbox: 5475.4873 (5475.0133)  loss_obj_bbox: 5536.4736 (5669.6492)  loss_sub_giou: 2.1606 (2.1895)  loss_obj_giou: 2.5704 (2.5703)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7222)  loss_verb_ce_unscaled: 50.0278 (52.7317)  loss_sub_bbox_unscaled: 1095.0974 (1095.0027)  loss_obj_bbox_unscaled: 1107.2947 (1133.9298)  loss_sub_giou_unscaled: 1.0803 (1.0947)  loss_obj_giou_unscaled: 1.2852 (1.2852)  obj_cardinality_error_unscaled: 46.5000 (45.9933)  time: 2.9509  data: 1.9503  max mem: 2876\n",
      "Epoch: [1]  [ 15/540]  eta: 0:25:38  lr: 0.001000  obj_class_error: 100.00  loss: 10978.3057 (11230.1569)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (53.4441)  loss_sub_bbox: 5359.9170 (5455.3648)  loss_obj_bbox: 5536.4736 (5712.0912)  loss_sub_giou: 2.1502 (2.1802)  loss_obj_giou: 2.5365 (2.5643)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7396)  loss_verb_ce_unscaled: 50.0278 (53.4441)  loss_sub_bbox_unscaled: 1071.9834 (1091.0730)  loss_obj_bbox_unscaled: 1107.2947 (1142.4182)  loss_sub_giou_unscaled: 1.0751 (1.0901)  loss_obj_giou_unscaled: 1.2682 (1.2822)  obj_cardinality_error_unscaled: 46.5000 (46.0250)  time: 2.9309  data: 1.9409  max mem: 2876\n",
      "Epoch: [1]  [ 16/540]  eta: 0:25:25  lr: 0.001000  obj_class_error: 100.00  loss: 11043.4570 (11289.9553)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 58.1599 (54.1717)  loss_sub_bbox: 5475.4873 (5461.9582)  loss_obj_bbox: 5763.9971 (5764.5510)  loss_sub_giou: 2.1606 (2.1921)  loss_obj_giou: 2.5704 (2.5700)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7549)  loss_verb_ce_unscaled: 58.1599 (54.1717)  loss_sub_bbox_unscaled: 1095.0974 (1092.3916)  loss_obj_bbox_unscaled: 1152.7994 (1152.9102)  loss_sub_giou_unscaled: 1.0803 (1.0961)  loss_obj_giou_unscaled: 1.2852 (1.2850)  obj_cardinality_error_unscaled: 46.5000 (46.0647)  time: 2.9115  data: 1.9277  max mem: 2876\n",
      "Epoch: [1]  [ 17/540]  eta: 0:25:19  lr: 0.001000  obj_class_error: 100.00  loss: 11043.4570 (11343.0356)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 58.1599 (54.8185)  loss_sub_bbox: 5475.4873 (5478.7027)  loss_obj_bbox: 5763.9971 (5800.2506)  loss_sub_giou: 2.1502 (2.1856)  loss_obj_giou: 2.5365 (2.5660)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7685)  loss_verb_ce_unscaled: 58.1599 (54.8185)  loss_sub_bbox_unscaled: 1095.0974 (1095.7405)  loss_obj_bbox_unscaled: 1152.7994 (1160.0501)  loss_sub_giou_unscaled: 1.0751 (1.0928)  loss_obj_giou_unscaled: 1.2682 (1.2830)  obj_cardinality_error_unscaled: 46.5000 (46.0667)  time: 2.9063  data: 1.9291  max mem: 2876\n",
      "Epoch: [1]  [ 18/540]  eta: 0:25:16  lr: 0.001000  obj_class_error: 100.00  loss: 11288.3369 (11359.0442)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 58.1657 (54.9955)  loss_sub_bbox: 5519.5044 (5495.6342)  loss_obj_bbox: 5779.4673 (5799.1567)  loss_sub_giou: 2.1606 (2.1889)  loss_obj_giou: 2.5365 (2.5567)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7807)  loss_verb_ce_unscaled: 58.1657 (54.9955)  loss_sub_bbox_unscaled: 1103.9009 (1099.1268)  loss_obj_bbox_unscaled: 1155.8934 (1159.8313)  loss_sub_giou_unscaled: 1.0803 (1.0944)  loss_obj_giou_unscaled: 1.2682 (1.2783)  obj_cardinality_error_unscaled: 46.5000 (46.1000)  time: 2.9050  data: 1.9337  max mem: 2876\n",
      "Epoch: [1]  [ 19/540]  eta: 0:25:11  lr: 0.001000  obj_class_error: 100.00  loss: 11043.4570 (11338.2084)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 58.1599 (54.4394)  loss_sub_bbox: 5475.4873 (5484.8074)  loss_obj_bbox: 5763.9971 (5789.7035)  loss_sub_giou: 2.1606 (2.1919)  loss_obj_giou: 2.5342 (2.5540)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7917)  loss_verb_ce_unscaled: 58.1599 (54.4394)  loss_sub_bbox_unscaled: 1095.0974 (1096.9615)  loss_obj_bbox_unscaled: 1152.7994 (1157.9407)  loss_sub_giou_unscaled: 1.0803 (1.0960)  loss_obj_giou_unscaled: 1.2671 (1.2770)  obj_cardinality_error_unscaled: 46.5000 (46.0350)  time: 2.9006  data: 1.9354  max mem: 2876\n",
      "Epoch: [1]  [ 20/540]  eta: 0:25:10  lr: 0.001000  obj_class_error: 100.00  loss: 11288.3369 (11372.6338)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (53.9006)  loss_sub_bbox: 5475.4873 (5521.4681)  loss_obj_bbox: 5763.9971 (5787.9960)  loss_sub_giou: 2.1610 (2.1979)  loss_obj_giou: 2.5342 (2.5590)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8016)  loss_verb_ce_unscaled: 50.0278 (53.9006)  loss_sub_bbox_unscaled: 1095.0974 (1104.2936)  loss_obj_bbox_unscaled: 1152.7994 (1157.5992)  loss_sub_giou_unscaled: 1.0805 (1.0989)  loss_obj_giou_unscaled: 1.2671 (1.2795)  obj_cardinality_error_unscaled: 46.2000 (46.0238)  time: 2.7874  data: 1.9390  max mem: 2876\n",
      "Epoch: [1]  [ 21/540]  eta: 0:25:06  lr: 0.001000  obj_class_error: 100.00  loss: 11288.3369 (11345.4301)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 49.0376 (53.6369)  loss_sub_bbox: 5475.4873 (5517.5065)  loss_obj_bbox: 5763.9971 (5765.0096)  loss_sub_giou: 2.1653 (2.1964)  loss_obj_giou: 2.5365 (2.5686)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8106)  loss_verb_ce_unscaled: 49.0376 (53.6369)  loss_sub_bbox_unscaled: 1095.0974 (1103.5013)  loss_obj_bbox_unscaled: 1152.7994 (1153.0019)  loss_sub_giou_unscaled: 1.0826 (1.0982)  loss_obj_giou_unscaled: 1.2682 (1.2843)  obj_cardinality_error_unscaled: 46.1000 (46.0273)  time: 2.7964  data: 1.9480  max mem: 2876\n",
      "Epoch: [1]  [ 22/540]  eta: 0:25:06  lr: 0.001000  obj_class_error: 100.00  loss: 11288.3369 (11307.4527)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (53.7217)  loss_sub_bbox: 5434.3115 (5502.7769)  loss_obj_bbox: 5763.9971 (5741.6752)  loss_sub_giou: 2.1653 (2.1994)  loss_obj_giou: 2.5365 (2.5675)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8188)  loss_verb_ce_unscaled: 50.0278 (53.7217)  loss_sub_bbox_unscaled: 1086.8623 (1100.5554)  loss_obj_bbox_unscaled: 1152.7994 (1148.3350)  loss_sub_giou_unscaled: 1.0826 (1.0997)  loss_obj_giou_unscaled: 1.2682 (1.2837)  obj_cardinality_error_unscaled: 46.2000 (46.0391)  time: 2.8084  data: 1.9580  max mem: 2876\n",
      "Epoch: [1]  [ 23/540]  eta: 0:25:01  lr: 0.001000  obj_class_error: 100.00  loss: 11288.3369 (11312.7353)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (53.7491)  loss_sub_bbox: 5434.3115 (5503.1350)  loss_obj_bbox: 5763.9971 (5746.5650)  loss_sub_giou: 2.1653 (2.2036)  loss_obj_giou: 2.5424 (2.5705)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8264)  loss_verb_ce_unscaled: 54.3813 (53.7491)  loss_sub_bbox_unscaled: 1086.8623 (1100.6270)  loss_obj_bbox_unscaled: 1152.7994 (1149.3130)  loss_sub_giou_unscaled: 1.0826 (1.1018)  loss_obj_giou_unscaled: 1.2712 (1.2852)  obj_cardinality_error_unscaled: 46.2000 (46.0417)  time: 2.8099  data: 1.9603  max mem: 2876\n",
      "Epoch: [1]  [ 24/540]  eta: 0:24:58  lr: 0.001000  obj_class_error: 100.00  loss: 11288.3369 (11288.7311)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (54.3024)  loss_sub_bbox: 5434.3115 (5485.7257)  loss_obj_bbox: 5753.8472 (5739.4147)  loss_sub_giou: 2.1653 (2.2050)  loss_obj_giou: 2.5875 (2.5711)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8333)  loss_verb_ce_unscaled: 54.3813 (54.3024)  loss_sub_bbox_unscaled: 1086.8623 (1097.1451)  loss_obj_bbox_unscaled: 1150.7694 (1147.8829)  loss_sub_giou_unscaled: 1.0826 (1.1025)  loss_obj_giou_unscaled: 1.2938 (1.2856)  obj_cardinality_error_unscaled: 46.2000 (46.0680)  time: 2.8148  data: 1.9608  max mem: 2876\n",
      "Epoch: [1]  [ 25/540]  eta: 0:24:53  lr: 0.001000  obj_class_error: 100.00  loss: 11434.2344 (11333.6569)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (54.6803)  loss_sub_bbox: 5475.4873 (5509.7258)  loss_obj_bbox: 5779.4673 (5759.9681)  loss_sub_giou: 2.1653 (2.2032)  loss_obj_giou: 2.5875 (2.5674)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8397)  loss_verb_ce_unscaled: 54.3813 (54.6803)  loss_sub_bbox_unscaled: 1095.0974 (1101.9452)  loss_obj_bbox_unscaled: 1155.8934 (1151.9936)  loss_sub_giou_unscaled: 1.0826 (1.1016)  loss_obj_giou_unscaled: 1.2938 (1.2837)  obj_cardinality_error_unscaled: 46.2000 (46.0923)  time: 2.8247  data: 1.9707  max mem: 2876\n",
      "Epoch: [1]  [ 26/540]  eta: 0:24:47  lr: 0.001000  obj_class_error: 100.00  loss: 11434.2344 (11337.9397)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (54.4366)  loss_sub_bbox: 5475.4873 (5529.4095)  loss_obj_bbox: 5753.8472 (5744.8061)  loss_sub_giou: 2.1855 (2.2031)  loss_obj_giou: 2.6079 (2.5722)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8457)  loss_verb_ce_unscaled: 50.0278 (54.4366)  loss_sub_bbox_unscaled: 1095.0974 (1105.8819)  loss_obj_bbox_unscaled: 1150.7694 (1148.9612)  loss_sub_giou_unscaled: 1.0928 (1.1016)  loss_obj_giou_unscaled: 1.3040 (1.2861)  obj_cardinality_error_unscaled: 46.1000 (46.0889)  time: 2.8327  data: 1.9767  max mem: 2876\n",
      "Epoch: [1]  [ 27/540]  eta: 0:24:39  lr: 0.001000  obj_class_error: 100.00  loss: 11449.2930 (11353.7779)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (54.1469)  loss_sub_bbox: 5475.4873 (5521.8826)  loss_obj_bbox: 5779.4673 (5768.4576)  loss_sub_giou: 2.2013 (2.2053)  loss_obj_giou: 2.6036 (2.5733)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8512)  loss_verb_ce_unscaled: 50.0278 (54.1469)  loss_sub_bbox_unscaled: 1095.0974 (1104.3765)  loss_obj_bbox_unscaled: 1155.8934 (1153.6915)  loss_sub_giou_unscaled: 1.1007 (1.1027)  loss_obj_giou_unscaled: 1.3018 (1.2867)  obj_cardinality_error_unscaled: 46.1000 (46.0679)  time: 2.8251  data: 1.9676  max mem: 2876\n",
      "Epoch: [1]  [ 28/540]  eta: 0:24:30  lr: 0.001000  obj_class_error: 100.00  loss: 11582.5156 (11377.1144)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (54.4360)  loss_sub_bbox: 5475.4873 (5539.7567)  loss_obj_bbox: 5859.0303 (5773.6281)  loss_sub_giou: 2.2374 (2.2073)  loss_obj_giou: 2.6001 (2.5743)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8563)  loss_verb_ce_unscaled: 50.0278 (54.4360)  loss_sub_bbox_unscaled: 1095.0974 (1107.9513)  loss_obj_bbox_unscaled: 1171.8060 (1154.7256)  loss_sub_giou_unscaled: 1.1187 (1.1037)  loss_obj_giou_unscaled: 1.3000 (1.2871)  obj_cardinality_error_unscaled: 46.1000 (46.0793)  time: 2.7997  data: 1.9425  max mem: 2876\n",
      "Epoch: [1]  [ 29/540]  eta: 0:24:30  lr: 0.001000  obj_class_error: 100.00  loss: 11582.5156 (11387.5429)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (54.6551)  loss_sub_bbox: 5475.4873 (5546.4165)  loss_obj_bbox: 5859.0303 (5777.1838)  loss_sub_giou: 2.2013 (2.2051)  loss_obj_giou: 2.5875 (2.5703)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8611)  loss_verb_ce_unscaled: 54.3813 (54.6551)  loss_sub_bbox_unscaled: 1095.0974 (1109.2833)  loss_obj_bbox_unscaled: 1171.8060 (1155.4368)  loss_sub_giou_unscaled: 1.1007 (1.1025)  loss_obj_giou_unscaled: 1.2938 (1.2852)  obj_cardinality_error_unscaled: 46.2000 (46.0900)  time: 2.8134  data: 1.9538  max mem: 2876\n",
      "Epoch: [1]  [ 30/540]  eta: 0:24:29  lr: 0.001000  obj_class_error: 100.00  loss: 11582.5156 (11396.2528)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (54.0963)  loss_sub_bbox: 5434.3115 (5539.4462)  loss_obj_bbox: 5859.0303 (5793.4242)  loss_sub_giou: 2.2141 (2.2054)  loss_obj_giou: 2.5424 (2.5686)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8656)  loss_verb_ce_unscaled: 54.3813 (54.0963)  loss_sub_bbox_unscaled: 1086.8623 (1107.8892)  loss_obj_bbox_unscaled: 1171.8060 (1158.6848)  loss_sub_giou_unscaled: 1.1070 (1.1027)  loss_obj_giou_unscaled: 1.2712 (1.2843)  obj_cardinality_error_unscaled: 46.1000 (46.0548)  time: 2.8245  data: 1.9625  max mem: 2876\n",
      "Epoch: [1]  [ 31/540]  eta: 0:24:25  lr: 0.001000  obj_class_error: 100.00  loss: 11582.5156 (11390.1138)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 50.0278 (53.8806)  loss_sub_bbox: 5434.3115 (5519.9471)  loss_obj_bbox: 5880.3003 (5806.9932)  loss_sub_giou: 2.2142 (2.2056)  loss_obj_giou: 2.5424 (2.5752)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8698)  loss_verb_ce_unscaled: 50.0278 (53.8806)  loss_sub_bbox_unscaled: 1086.8623 (1103.9894)  loss_obj_bbox_unscaled: 1176.0601 (1161.3986)  loss_sub_giou_unscaled: 1.1071 (1.1028)  loss_obj_giou_unscaled: 1.2712 (1.2876)  obj_cardinality_error_unscaled: 46.1000 (46.0500)  time: 2.8179  data: 1.9550  max mem: 2876\n",
      "Epoch: [1]  [ 32/540]  eta: 0:24:17  lr: 0.001000  obj_class_error: 100.00  loss: 11582.5156 (11329.4538)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (53.9321)  loss_sub_bbox: 5434.3115 (5490.0564)  loss_obj_bbox: 5859.0303 (5776.1742)  loss_sub_giou: 2.2142 (2.2028)  loss_obj_giou: 2.5875 (2.5763)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8737)  loss_verb_ce_unscaled: 54.3813 (53.9321)  loss_sub_bbox_unscaled: 1086.8623 (1098.0113)  loss_obj_bbox_unscaled: 1171.8060 (1155.2348)  loss_sub_giou_unscaled: 1.1071 (1.1014)  loss_obj_giou_unscaled: 1.2938 (1.2882)  obj_cardinality_error_unscaled: 46.1000 (46.0606)  time: 2.8097  data: 1.9477  max mem: 2876\n",
      "Epoch: [1]  [ 33/540]  eta: 0:24:11  lr: 0.001000  obj_class_error: 100.00  loss: 11449.2930 (11323.0701)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.0974)  loss_sub_bbox: 5434.3115 (5499.1027)  loss_obj_bbox: 5779.4673 (5760.5814)  loss_sub_giou: 2.2141 (2.2027)  loss_obj_giou: 2.5424 (2.5738)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8775)  loss_verb_ce_unscaled: 55.5769 (54.0974)  loss_sub_bbox_unscaled: 1086.8623 (1099.8205)  loss_obj_bbox_unscaled: 1155.8934 (1152.1163)  loss_sub_giou_unscaled: 1.1070 (1.1014)  loss_obj_giou_unscaled: 1.2712 (1.2869)  obj_cardinality_error_unscaled: 46.1000 (46.0676)  time: 2.8071  data: 1.9437  max mem: 2876\n",
      "Epoch: [1]  [ 34/540]  eta: 0:24:08  lr: 0.001000  obj_class_error: 100.00  loss: 11449.2930 (11311.5711)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5867 (54.3842)  loss_sub_bbox: 5511.3721 (5515.1967)  loss_obj_bbox: 5779.4673 (5732.7061)  loss_sub_giou: 2.2141 (2.2024)  loss_obj_giou: 2.5424 (2.5697)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8810)  loss_verb_ce_unscaled: 55.5867 (54.3842)  loss_sub_bbox_unscaled: 1102.2744 (1103.0393)  loss_obj_bbox_unscaled: 1155.8934 (1146.5412)  loss_sub_giou_unscaled: 1.1070 (1.1012)  loss_obj_giou_unscaled: 1.2712 (1.2849)  obj_cardinality_error_unscaled: 46.3000 (46.0829)  time: 2.7961  data: 1.9321  max mem: 2876\n",
      "Epoch: [1]  [ 35/540]  eta: 0:24:03  lr: 0.001000  obj_class_error: 100.00  loss: 11449.2930 (11316.3529)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.1844)  loss_sub_bbox: 5567.4521 (5528.1797)  loss_obj_bbox: 5753.8472 (5724.6972)  loss_sub_giou: 2.2141 (2.2021)  loss_obj_giou: 2.5875 (2.5774)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8843)  loss_verb_ce_unscaled: 55.5769 (54.1844)  loss_sub_bbox_unscaled: 1113.4905 (1105.6359)  loss_obj_bbox_unscaled: 1150.7694 (1144.9394)  loss_sub_giou_unscaled: 1.1070 (1.1010)  loss_obj_giou_unscaled: 1.2938 (1.2887)  obj_cardinality_error_unscaled: 46.3000 (46.0972)  time: 2.7998  data: 1.9323  max mem: 2876\n",
      "Epoch: [1]  [ 36/540]  eta: 0:23:58  lr: 0.001000  obj_class_error: 100.00  loss: 11434.2344 (11276.1047)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (54.0453)  loss_sub_bbox: 5511.3721 (5515.9791)  loss_obj_bbox: 5610.0923 (5696.7874)  loss_sub_giou: 2.2080 (2.2023)  loss_obj_giou: 2.5875 (2.5785)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8874)  loss_verb_ce_unscaled: 54.3813 (54.0453)  loss_sub_bbox_unscaled: 1102.2744 (1103.1958)  loss_obj_bbox_unscaled: 1122.0184 (1139.3575)  loss_sub_giou_unscaled: 1.1040 (1.1011)  loss_obj_giou_unscaled: 1.2938 (1.2893)  obj_cardinality_error_unscaled: 46.1000 (46.0892)  time: 2.8045  data: 1.9382  max mem: 2876\n",
      "Epoch: [1]  [ 37/540]  eta: 0:23:54  lr: 0.001000  obj_class_error: 100.00  loss: 11434.2344 (11283.4846)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (54.1536)  loss_sub_bbox: 5434.3115 (5511.6233)  loss_obj_bbox: 5610.0923 (5708.4236)  loss_sub_giou: 2.2080 (2.2007)  loss_obj_giou: 2.5875 (2.5712)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8904)  loss_verb_ce_unscaled: 54.3813 (54.1536)  loss_sub_bbox_unscaled: 1086.8623 (1102.3247)  loss_obj_bbox_unscaled: 1122.0184 (1141.6847)  loss_sub_giou_unscaled: 1.1040 (1.1004)  loss_obj_giou_unscaled: 1.2938 (1.2856)  obj_cardinality_error_unscaled: 46.3000 (46.1000)  time: 2.8038  data: 1.9375  max mem: 2876\n",
      "Epoch: [1]  [ 38/540]  eta: 0:23:50  lr: 0.001000  obj_class_error: 100.00  loss: 11434.2344 (11316.3182)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (54.4096)  loss_sub_bbox: 5434.3115 (5530.2849)  loss_obj_bbox: 5610.0923 (5722.3376)  loss_sub_giou: 2.2080 (2.2045)  loss_obj_giou: 2.5875 (2.5696)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8932)  loss_verb_ce_unscaled: 54.3813 (54.4096)  loss_sub_bbox_unscaled: 1086.8623 (1106.0570)  loss_obj_bbox_unscaled: 1122.0184 (1144.4675)  loss_sub_giou_unscaled: 1.1040 (1.1022)  loss_obj_giou_unscaled: 1.2938 (1.2848)  obj_cardinality_error_unscaled: 46.3000 (46.1103)  time: 2.7989  data: 1.9278  max mem: 2876\n",
      "Epoch: [1]  [ 39/540]  eta: 0:23:45  lr: 0.001000  obj_class_error: 100.00  loss: 11434.2344 (11305.5275)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (54.2518)  loss_sub_bbox: 5434.3115 (5525.0891)  loss_obj_bbox: 5567.8086 (5716.9002)  loss_sub_giou: 2.2015 (2.2030)  loss_obj_giou: 2.6001 (2.5713)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8958)  loss_verb_ce_unscaled: 54.3813 (54.2518)  loss_sub_bbox_unscaled: 1086.8623 (1105.0178)  loss_obj_bbox_unscaled: 1113.5618 (1143.3800)  loss_sub_giou_unscaled: 1.1008 (1.1015)  loss_obj_giou_unscaled: 1.3000 (1.2857)  obj_cardinality_error_unscaled: 46.3000 (46.1025)  time: 2.7903  data: 1.9197  max mem: 2876\n",
      "Epoch: [1]  [ 40/540]  eta: 0:23:40  lr: 0.001000  obj_class_error: 100.00  loss: 11199.8057 (11282.8695)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.3811)  loss_sub_bbox: 5350.4614 (5517.2009)  loss_obj_bbox: 5504.8438 (5702.0053)  loss_sub_giou: 2.2013 (2.2001)  loss_obj_giou: 2.5875 (2.5699)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8984)  loss_verb_ce_unscaled: 55.5769 (54.3811)  loss_sub_bbox_unscaled: 1070.0923 (1103.4402)  loss_obj_bbox_unscaled: 1100.9688 (1140.4010)  loss_sub_giou_unscaled: 1.1007 (1.1001)  loss_obj_giou_unscaled: 1.2938 (1.2850)  obj_cardinality_error_unscaled: 46.3000 (46.1195)  time: 2.7748  data: 1.9031  max mem: 2876\n",
      "Epoch: [1]  [ 41/540]  eta: 0:23:35  lr: 0.001000  obj_class_error: 100.00  loss: 11434.2344 (11312.8598)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.2539)  loss_sub_bbox: 5350.4614 (5528.9219)  loss_obj_bbox: 5567.8086 (5720.4003)  loss_sub_giou: 2.2013 (2.1998)  loss_obj_giou: 2.5875 (2.5717)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.9008)  loss_verb_ce_unscaled: 55.5769 (54.2539)  loss_sub_bbox_unscaled: 1070.0923 (1105.7844)  loss_obj_bbox_unscaled: 1113.5618 (1144.0801)  loss_sub_giou_unscaled: 1.1007 (1.0999)  loss_obj_giou_unscaled: 1.2938 (1.2859)  obj_cardinality_error_unscaled: 46.3000 (46.1119)  time: 2.7640  data: 1.8922  max mem: 2876\n",
      "Epoch: [1]  [ 42/540]  eta: 0:23:33  lr: 0.001000  obj_class_error: 100.00  loss: 11434.2344 (11301.6473)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3813 (53.9457)  loss_sub_bbox: 5350.4614 (5515.1030)  loss_obj_bbox: 5845.6953 (5723.3142)  loss_sub_giou: 2.1927 (2.1995)  loss_obj_giou: 2.6001 (2.5729)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.9031)  loss_verb_ce_unscaled: 54.3813 (53.9457)  loss_sub_bbox_unscaled: 1070.0923 (1103.0206)  loss_obj_bbox_unscaled: 1169.1390 (1144.6628)  loss_sub_giou_unscaled: 1.0964 (1.0997)  loss_obj_giou_unscaled: 1.3000 (1.2865)  obj_cardinality_error_unscaled: 46.3000 (46.1047)  time: 2.7590  data: 1.8887  max mem: 2876\n",
      "Epoch: [1]  [ 43/540]  eta: 0:23:31  lr: 0.001000  obj_class_error: 96.15  loss: 11199.8057 (11298.4875)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.2157)  loss_sub_bbox: 5350.4614 (5518.3293)  loss_obj_bbox: 5567.8086 (5716.6597)  loss_sub_giou: 2.1927 (2.2004)  loss_obj_giou: 2.5875 (2.5703)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8179)  loss_verb_ce_unscaled: 55.5769 (54.2157)  loss_sub_bbox_unscaled: 1070.0923 (1103.6659)  loss_obj_bbox_unscaled: 1113.5618 (1143.3319)  loss_sub_giou_unscaled: 1.0964 (1.1002)  loss_obj_giou_unscaled: 1.2938 (1.2851)  obj_cardinality_error_unscaled: 46.4000 (46.1114)  time: 2.7640  data: 1.8972  max mem: 2876\n",
      "Epoch: [1]  [ 44/540]  eta: 0:23:26  lr: 0.001000  obj_class_error: 100.00  loss: 11199.8057 (11286.7794)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.2461)  loss_sub_bbox: 5350.4614 (5508.7217)  loss_obj_bbox: 5620.9424 (5714.5326)  loss_sub_giou: 2.1897 (2.1982)  loss_obj_giou: 2.5150 (2.5687)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8219)  loss_verb_ce_unscaled: 55.5769 (54.2461)  loss_sub_bbox_unscaled: 1070.0923 (1101.7443)  loss_obj_bbox_unscaled: 1124.1885 (1142.9065)  loss_sub_giou_unscaled: 1.0949 (1.0991)  loss_obj_giou_unscaled: 1.2575 (1.2843)  obj_cardinality_error_unscaled: 46.4000 (46.1200)  time: 2.7482  data: 1.8850  max mem: 2876\n",
      "Epoch: [1]  [ 45/540]  eta: 0:23:19  lr: 0.001000  obj_class_error: 100.00  loss: 11162.6123 (11255.9321)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.2751)  loss_sub_bbox: 5330.3394 (5490.8937)  loss_obj_bbox: 5504.8438 (5701.4903)  loss_sub_giou: 2.1897 (2.1958)  loss_obj_giou: 2.5150 (2.5652)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8258)  loss_verb_ce_unscaled: 55.5769 (54.2751)  loss_sub_bbox_unscaled: 1066.0679 (1098.1787)  loss_obj_bbox_unscaled: 1100.9688 (1140.2981)  loss_sub_giou_unscaled: 1.0949 (1.0979)  loss_obj_giou_unscaled: 1.2575 (1.2826)  obj_cardinality_error_unscaled: 46.4000 (46.1304)  time: 2.7327  data: 1.8691  max mem: 2876\n",
      "Epoch: [1]  [ 46/540]  eta: 0:23:13  lr: 0.001000  obj_class_error: 100.00  loss: 11162.6123 (11275.5763)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5802 (54.7328)  loss_sub_bbox: 5330.3394 (5492.2596)  loss_obj_bbox: 5620.9424 (5719.3043)  loss_sub_giou: 2.1897 (2.1982)  loss_obj_giou: 2.5150 (2.5693)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8295)  loss_verb_ce_unscaled: 55.5802 (54.7328)  loss_sub_bbox_unscaled: 1066.0679 (1098.4519)  loss_obj_bbox_unscaled: 1124.1885 (1143.8609)  loss_sub_giou_unscaled: 1.0949 (1.0991)  loss_obj_giou_unscaled: 1.2575 (1.2846)  obj_cardinality_error_unscaled: 46.4000 (46.1426)  time: 2.7231  data: 1.8571  max mem: 2876\n",
      "Epoch: [1]  [ 47/540]  eta: 0:23:08  lr: 0.001000  obj_class_error: 97.30  loss: 11112.4062 (11265.7372)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5802 (54.3821)  loss_sub_bbox: 5330.3394 (5483.7187)  loss_obj_bbox: 5620.9424 (5718.3568)  loss_sub_giou: 2.1897 (2.1983)  loss_obj_giou: 2.5150 (2.5691)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7768)  loss_verb_ce_unscaled: 55.5802 (54.3821)  loss_sub_bbox_unscaled: 1066.0679 (1096.7437)  loss_obj_bbox_unscaled: 1124.1885 (1143.6714)  loss_sub_giou_unscaled: 1.0949 (1.0991)  loss_obj_giou_unscaled: 1.2575 (1.2846)  obj_cardinality_error_unscaled: 46.4000 (46.1250)  time: 2.7210  data: 1.8555  max mem: 2876\n",
      "Epoch: [1]  [ 48/540]  eta: 0:23:02  lr: 0.001000  obj_class_error: 100.00  loss: 11112.4062 (11262.6087)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.0016)  loss_sub_bbox: 5322.4507 (5474.2095)  loss_obj_bbox: 5620.9424 (5725.1157)  loss_sub_giou: 2.1865 (2.1968)  loss_obj_giou: 2.5150 (2.5729)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7813)  loss_verb_ce_unscaled: 55.5769 (54.0016)  loss_sub_bbox_unscaled: 1064.4901 (1094.8419)  loss_obj_bbox_unscaled: 1124.1885 (1145.0231)  loss_sub_giou_unscaled: 1.0932 (1.0984)  loss_obj_giou_unscaled: 1.2575 (1.2864)  obj_cardinality_error_unscaled: 46.4000 (46.1102)  time: 2.7213  data: 1.8527  max mem: 2876\n",
      "Epoch: [1]  [ 49/540]  eta: 0:22:58  lr: 0.001000  obj_class_error: 100.00  loss: 11112.4062 (11287.2677)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (54.0332)  loss_sub_bbox: 5322.4507 (5488.0739)  loss_obj_bbox: 5620.9424 (5735.8745)  loss_sub_giou: 2.1897 (2.1998)  loss_obj_giou: 2.5630 (2.5742)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7857)  loss_verb_ce_unscaled: 55.5769 (54.0332)  loss_sub_bbox_unscaled: 1064.4901 (1097.6148)  loss_obj_bbox_unscaled: 1124.1885 (1147.1749)  loss_sub_giou_unscaled: 1.0949 (1.0999)  loss_obj_giou_unscaled: 1.2815 (1.2871)  obj_cardinality_error_unscaled: 46.3000 (46.1020)  time: 2.7046  data: 1.8295  max mem: 2876\n",
      "Epoch: [1]  [ 50/540]  eta: 0:22:52  lr: 0.001000  obj_class_error: 100.00  loss: 11022.7275 (11282.0807)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5769 (53.9745)  loss_sub_bbox: 5322.4507 (5496.6461)  loss_obj_bbox: 5504.8438 (5722.1767)  loss_sub_giou: 2.1865 (2.1995)  loss_obj_giou: 2.5630 (2.5719)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7899)  loss_verb_ce_unscaled: 55.5769 (53.9745)  loss_sub_bbox_unscaled: 1064.4901 (1099.3292)  loss_obj_bbox_unscaled: 1100.9688 (1144.4353)  loss_sub_giou_unscaled: 1.0932 (1.0997)  loss_obj_giou_unscaled: 1.2815 (1.2859)  obj_cardinality_error_unscaled: 46.3000 (46.1059)  time: 2.6775  data: 1.8068  max mem: 2876\n",
      "Epoch: [1]  [ 51/540]  eta: 0:22:49  lr: 0.001000  obj_class_error: 100.00  loss: 11022.7275 (11294.5702)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 55.5769 (53.9181)  loss_sub_bbox: 5350.4614 (5499.7750)  loss_obj_bbox: 5504.8438 (5731.5909)  loss_sub_giou: 2.1849 (2.1990)  loss_obj_giou: 2.5630 (2.5750)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7939)  loss_verb_ce_unscaled: 55.5769 (53.9181)  loss_sub_bbox_unscaled: 1070.0923 (1099.9550)  loss_obj_bbox_unscaled: 1100.9688 (1146.3182)  loss_sub_giou_unscaled: 1.0924 (1.0995)  loss_obj_giou_unscaled: 1.2815 (1.2875)  obj_cardinality_error_unscaled: 46.4000 (46.1115)  time: 2.6743  data: 1.8032  max mem: 2876\n",
      "Epoch: [1]  [ 52/540]  eta: 0:22:44  lr: 0.001000  obj_class_error: 100.00  loss: 11022.7275 (11272.7780)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 55.5801 (54.1764)  loss_sub_bbox: 5408.0542 (5498.0444)  loss_obj_bbox: 5504.8438 (5711.2717)  loss_sub_giou: 2.1849 (2.1977)  loss_obj_giou: 2.5630 (2.5756)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7978)  loss_verb_ce_unscaled: 55.5801 (54.1764)  loss_sub_bbox_unscaled: 1081.6108 (1099.6089)  loss_obj_bbox_unscaled: 1100.9688 (1142.2543)  loss_sub_giou_unscaled: 1.0924 (1.0988)  loss_obj_giou_unscaled: 1.2815 (1.2878)  obj_cardinality_error_unscaled: 46.4000 (46.1302)  time: 2.6760  data: 1.8045  max mem: 2876\n",
      "Epoch: [1]  [ 53/540]  eta: 0:22:38  lr: 0.001000  obj_class_error: 100.00  loss: 10920.6055 (11264.7521)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 55.5801 (54.2025)  loss_sub_bbox: 5408.0542 (5499.7583)  loss_obj_bbox: 5504.8438 (5701.5077)  loss_sub_giou: 2.1841 (2.1961)  loss_obj_giou: 2.5630 (2.5753)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8016)  loss_verb_ce_unscaled: 55.5801 (54.2025)  loss_sub_bbox_unscaled: 1081.6108 (1099.9517)  loss_obj_bbox_unscaled: 1100.9688 (1140.3015)  loss_sub_giou_unscaled: 1.0920 (1.0980)  loss_obj_giou_unscaled: 1.2815 (1.2877)  obj_cardinality_error_unscaled: 46.4000 (46.1370)  time: 2.6609  data: 1.7919  max mem: 2876\n",
      "Epoch: [1]  [ 54/540]  eta: 0:22:32  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11258.6441)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 51.0412 (54.0590)  loss_sub_bbox: 5408.0542 (5501.3045)  loss_obj_bbox: 5504.8438 (5693.9972)  loss_sub_giou: 2.1841 (2.1972)  loss_obj_giou: 2.5630 (2.5740)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8052)  loss_verb_ce_unscaled: 51.0412 (54.0590)  loss_sub_bbox_unscaled: 1081.6108 (1100.2609)  loss_obj_bbox_unscaled: 1100.9688 (1138.7994)  loss_sub_giou_unscaled: 1.0920 (1.0986)  loss_obj_giou_unscaled: 1.2815 (1.2870)  obj_cardinality_error_unscaled: 46.4000 (46.1364)  time: 2.6435  data: 1.7745  max mem: 2876\n",
      "Epoch: [1]  [ 55/540]  eta: 0:22:29  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11271.8696)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 52.1060 (54.0241)  loss_sub_bbox: 5408.0542 (5505.8988)  loss_obj_bbox: 5620.9424 (5702.6631)  loss_sub_giou: 2.1841 (2.1972)  loss_obj_giou: 2.5630 (2.5743)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8087)  loss_verb_ce_unscaled: 52.1060 (54.0241)  loss_sub_bbox_unscaled: 1081.6108 (1101.1798)  loss_obj_bbox_unscaled: 1124.1885 (1140.5326)  loss_sub_giou_unscaled: 1.0920 (1.0986)  loss_obj_giou_unscaled: 1.2815 (1.2871)  obj_cardinality_error_unscaled: 46.3000 (46.1339)  time: 2.6439  data: 1.7755  max mem: 2876\n",
      "Epoch: [1]  [ 56/540]  eta: 0:22:25  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11258.4153)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 54.3770 (54.0303)  loss_sub_bbox: 5408.0542 (5500.9007)  loss_obj_bbox: 5620.9424 (5694.2040)  loss_sub_giou: 2.1753 (2.1958)  loss_obj_giou: 2.5629 (2.5723)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8120)  loss_verb_ce_unscaled: 54.3770 (54.0303)  loss_sub_bbox_unscaled: 1081.6108 (1100.1801)  loss_obj_bbox_unscaled: 1124.1885 (1138.8408)  loss_sub_giou_unscaled: 1.0877 (1.0979)  loss_obj_giou_unscaled: 1.2814 (1.2861)  obj_cardinality_error_unscaled: 46.3000 (46.1333)  time: 2.6416  data: 1.7656  max mem: 2876\n",
      "Epoch: [1]  [ 57/540]  eta: 0:22:20  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11276.4603)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 52.1060 (53.8974)  loss_sub_bbox: 5555.0908 (5512.9359)  loss_obj_bbox: 5620.9424 (5700.3423)  loss_sub_giou: 2.1841 (2.1989)  loss_obj_giou: 2.5630 (2.5736)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8152)  loss_verb_ce_unscaled: 52.1060 (53.8974)  loss_sub_bbox_unscaled: 1111.0182 (1102.5872)  loss_obj_bbox_unscaled: 1124.1885 (1140.0685)  loss_sub_giou_unscaled: 1.0920 (1.0995)  loss_obj_giou_unscaled: 1.2815 (1.2868)  obj_cardinality_error_unscaled: 46.1000 (46.1172)  time: 2.6306  data: 1.7546  max mem: 2876\n",
      "Epoch: [1]  [ 58/540]  eta: 0:22:18  lr: 0.001000  obj_class_error: 96.00  loss: 10884.6895 (11265.5973)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 52.1060 (54.0709)  loss_sub_bbox: 5408.0542 (5508.0975)  loss_obj_bbox: 5504.8438 (5694.1493)  loss_sub_giou: 2.1753 (2.1965)  loss_obj_giou: 2.5630 (2.5710)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7506)  loss_verb_ce_unscaled: 52.1060 (54.0709)  loss_sub_bbox_unscaled: 1081.6108 (1101.6195)  loss_obj_bbox_unscaled: 1100.9688 (1138.8299)  loss_sub_giou_unscaled: 1.0877 (1.0982)  loss_obj_giou_unscaled: 1.2815 (1.2855)  obj_cardinality_error_unscaled: 46.1000 (46.1237)  time: 2.6313  data: 1.7619  max mem: 2876\n",
      "Epoch: [1]  [ 59/540]  eta: 0:22:13  lr: 0.001000  obj_class_error: 100.00  loss: 10864.9131 (11258.9193)  loss_obj_ce: 4.5123 (4.5124)  loss_verb_ce: 52.1060 (54.0034)  loss_sub_bbox: 5408.0542 (5500.2571)  loss_obj_bbox: 5620.9424 (5695.3778)  loss_sub_giou: 2.1753 (2.1960)  loss_obj_giou: 2.5630 (2.5729)  loss_obj_ce_unscaled: 4.5123 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7547)  loss_verb_ce_unscaled: 52.1060 (54.0034)  loss_sub_bbox_unscaled: 1081.6108 (1100.0514)  loss_obj_bbox_unscaled: 1124.1885 (1139.0756)  loss_sub_giou_unscaled: 1.0877 (1.0980)  loss_obj_giou_unscaled: 1.2815 (1.2865)  obj_cardinality_error_unscaled: 46.1000 (46.1233)  time: 2.6274  data: 1.7558  max mem: 2876\n",
      "Epoch: [1]  [ 60/540]  eta: 0:22:09  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11277.8253)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 52.1060 (54.0094)  loss_sub_bbox: 5555.0908 (5510.1823)  loss_obj_bbox: 5673.8242 (5704.3501)  loss_sub_giou: 2.1841 (2.1965)  loss_obj_giou: 2.5907 (2.5748)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7588)  loss_verb_ce_unscaled: 52.1060 (54.0094)  loss_sub_bbox_unscaled: 1111.0182 (1102.0365)  loss_obj_bbox_unscaled: 1134.7649 (1140.8700)  loss_sub_giou_unscaled: 1.0920 (1.0983)  loss_obj_giou_unscaled: 1.2953 (1.2874)  obj_cardinality_error_unscaled: 46.1000 (46.1262)  time: 2.6251  data: 1.7557  max mem: 2876\n",
      "Epoch: [1]  [ 61/540]  eta: 0:22:09  lr: 0.001000  obj_class_error: 100.00  loss: 10864.9131 (11264.8364)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.3248)  loss_sub_bbox: 5408.0542 (5504.4772)  loss_obj_bbox: 5620.9424 (5696.7512)  loss_sub_giou: 2.1788 (2.1962)  loss_obj_giou: 2.5764 (2.5748)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7627)  loss_verb_ce_unscaled: 54.3694 (54.3248)  loss_sub_bbox_unscaled: 1081.6108 (1100.8954)  loss_obj_bbox_unscaled: 1124.1885 (1139.3502)  loss_sub_giou_unscaled: 1.0894 (1.0981)  loss_obj_giou_unscaled: 1.2882 (1.2874)  obj_cardinality_error_unscaled: 46.3000 (46.1339)  time: 2.6440  data: 1.7753  max mem: 2876\n",
      "Epoch: [1]  [ 62/540]  eta: 0:22:06  lr: 0.001000  obj_class_error: 100.00  loss: 10864.9131 (11256.9100)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3770 (54.4077)  loss_sub_bbox: 5408.0542 (5497.0993)  loss_obj_bbox: 5620.9424 (5696.1194)  loss_sub_giou: 2.1753 (2.1954)  loss_obj_giou: 2.5764 (2.5761)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7664)  loss_verb_ce_unscaled: 54.3770 (54.4077)  loss_sub_bbox_unscaled: 1081.6108 (1099.4199)  loss_obj_bbox_unscaled: 1124.1885 (1139.2239)  loss_sub_giou_unscaled: 1.0877 (1.0977)  loss_obj_giou_unscaled: 1.2882 (1.2880)  obj_cardinality_error_unscaled: 46.3000 (46.1413)  time: 2.6404  data: 1.7720  max mem: 2876\n",
      "Epoch: [1]  [ 63/540]  eta: 0:22:03  lr: 0.001000  obj_class_error: 100.00  loss: 10864.9131 (11262.6537)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3770 (54.4261)  loss_sub_bbox: 5408.0542 (5499.9589)  loss_obj_bbox: 5656.9487 (5698.9883)  loss_sub_giou: 2.1657 (2.1936)  loss_obj_giou: 2.5764 (2.5745)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7701)  loss_verb_ce_unscaled: 54.3770 (54.4261)  loss_sub_bbox_unscaled: 1081.6108 (1099.9918)  loss_obj_bbox_unscaled: 1131.3898 (1139.7977)  loss_sub_giou_unscaled: 1.0828 (1.0968)  loss_obj_giou_unscaled: 1.2882 (1.2873)  obj_cardinality_error_unscaled: 46.3000 (46.1406)  time: 2.6313  data: 1.7610  max mem: 2876\n",
      "Epoch: [1]  [ 64/540]  eta: 0:21:59  lr: 0.001000  obj_class_error: 100.00  loss: 10864.9131 (11253.2924)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3770 (54.4438)  loss_sub_bbox: 5408.0542 (5494.9810)  loss_obj_bbox: 5656.9487 (5694.5841)  loss_sub_giou: 2.1753 (2.1953)  loss_obj_giou: 2.5907 (2.5761)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7736)  loss_verb_ce_unscaled: 54.3770 (54.4438)  loss_sub_bbox_unscaled: 1081.6108 (1098.9962)  loss_obj_bbox_unscaled: 1131.3898 (1138.9168)  loss_sub_giou_unscaled: 1.0877 (1.0976)  loss_obj_giou_unscaled: 1.2953 (1.2880)  obj_cardinality_error_unscaled: 46.3000 (46.1477)  time: 2.6305  data: 1.7606  max mem: 2876\n",
      "Epoch: [1]  [ 65/540]  eta: 0:21:54  lr: 0.001000  obj_class_error: 100.00  loss: 10864.9131 (11234.2435)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3770 (54.8412)  loss_sub_bbox: 5408.0542 (5488.0558)  loss_obj_bbox: 5656.9487 (5682.0698)  loss_sub_giou: 2.1753 (2.1932)  loss_obj_giou: 2.5907 (2.5713)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7770)  loss_verb_ce_unscaled: 54.3770 (54.8412)  loss_sub_bbox_unscaled: 1081.6108 (1097.6112)  loss_obj_bbox_unscaled: 1131.3898 (1136.4140)  loss_sub_giou_unscaled: 1.0877 (1.0966)  loss_obj_giou_unscaled: 1.2953 (1.2857)  obj_cardinality_error_unscaled: 46.3000 (46.1591)  time: 2.6325  data: 1.7620  max mem: 2876\n",
      "Epoch: [1]  [ 66/540]  eta: 0:21:53  lr: 0.001000  obj_class_error: 100.00  loss: 10864.9131 (11237.5977)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.6152)  loss_sub_bbox: 5227.4717 (5481.7836)  loss_obj_bbox: 5656.9487 (5691.9235)  loss_sub_giou: 2.1657 (2.1926)  loss_obj_giou: 2.5764 (2.5706)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7804)  loss_verb_ce_unscaled: 54.3694 (54.6152)  loss_sub_bbox_unscaled: 1045.4944 (1096.3567)  loss_obj_bbox_unscaled: 1131.3898 (1138.3847)  loss_sub_giou_unscaled: 1.0828 (1.0963)  loss_obj_giou_unscaled: 1.2882 (1.2853)  obj_cardinality_error_unscaled: 46.1000 (46.1493)  time: 2.6504  data: 1.7822  max mem: 2876\n",
      "Epoch: [1]  [ 67/540]  eta: 0:21:49  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11256.9334)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3770 (54.6877)  loss_sub_bbox: 5408.0542 (5485.2849)  loss_obj_bbox: 5656.9487 (5707.6854)  loss_sub_giou: 2.1548 (2.1918)  loss_obj_giou: 2.5907 (2.5714)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7836)  loss_verb_ce_unscaled: 54.3770 (54.6877)  loss_sub_bbox_unscaled: 1081.6108 (1097.0570)  loss_obj_bbox_unscaled: 1131.3898 (1141.5371)  loss_sub_giou_unscaled: 1.0774 (1.0959)  loss_obj_giou_unscaled: 1.2953 (1.2857)  obj_cardinality_error_unscaled: 46.3000 (46.1544)  time: 2.6557  data: 1.7874  max mem: 2876\n",
      "Epoch: [1]  [ 68/540]  eta: 0:21:46  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11255.8979)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 55.5757 (54.9615)  loss_sub_bbox: 5408.0542 (5476.9724)  loss_obj_bbox: 5656.9487 (5714.6875)  loss_sub_giou: 2.1657 (2.1937)  loss_obj_giou: 2.5764 (2.5706)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7867)  loss_verb_ce_unscaled: 55.5757 (54.9615)  loss_sub_bbox_unscaled: 1081.6108 (1095.3945)  loss_obj_bbox_unscaled: 1131.3898 (1142.9375)  loss_sub_giou_unscaled: 1.0828 (1.0969)  loss_obj_giou_unscaled: 1.2882 (1.2853)  obj_cardinality_error_unscaled: 46.3000 (46.1638)  time: 2.6623  data: 1.7963  max mem: 2876\n",
      "Epoch: [1]  [ 69/540]  eta: 0:21:42  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11255.7107)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3770 (54.6941)  loss_sub_bbox: 5227.4717 (5472.2806)  loss_obj_bbox: 5656.9487 (5719.4585)  loss_sub_giou: 2.1657 (2.1934)  loss_obj_giou: 2.5764 (2.5718)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7898)  loss_verb_ce_unscaled: 54.3770 (54.6941)  loss_sub_bbox_unscaled: 1045.4944 (1094.4561)  loss_obj_bbox_unscaled: 1131.3898 (1143.8917)  loss_sub_giou_unscaled: 1.0828 (1.0967)  loss_obj_giou_unscaled: 1.2882 (1.2859)  obj_cardinality_error_unscaled: 46.3000 (46.1429)  time: 2.6580  data: 1.8002  max mem: 2876\n",
      "Epoch: [1]  [ 70/540]  eta: 0:21:38  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11262.4631)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3770 (54.5528)  loss_sub_bbox: 5227.4717 (5479.9785)  loss_obj_bbox: 5662.2812 (5718.6532)  loss_sub_giou: 2.1657 (2.1959)  loss_obj_giou: 2.5764 (2.5705)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7927)  loss_verb_ce_unscaled: 54.3770 (54.5528)  loss_sub_bbox_unscaled: 1045.4944 (1095.9957)  loss_obj_bbox_unscaled: 1132.4563 (1143.7306)  loss_sub_giou_unscaled: 1.0828 (1.0979)  loss_obj_giou_unscaled: 1.2882 (1.2853)  obj_cardinality_error_unscaled: 46.3000 (46.1380)  time: 2.6666  data: 1.8075  max mem: 2876\n",
      "Epoch: [1]  [ 71/540]  eta: 0:21:36  lr: 0.001000  obj_class_error: 100.00  loss: 10928.8076 (11268.5375)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3770 (54.4384)  loss_sub_bbox: 5221.0083 (5476.1027)  loss_obj_bbox: 5662.2812 (5728.7153)  loss_sub_giou: 2.1657 (2.1964)  loss_obj_giou: 2.5764 (2.5725)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7956)  loss_verb_ce_unscaled: 54.3770 (54.4384)  loss_sub_bbox_unscaled: 1044.2017 (1095.2205)  loss_obj_bbox_unscaled: 1132.4563 (1145.7431)  loss_sub_giou_unscaled: 1.0828 (1.0982)  loss_obj_giou_unscaled: 1.2882 (1.2863)  obj_cardinality_error_unscaled: 46.1000 (46.1347)  time: 2.6689  data: 1.8108  max mem: 2876\n",
      "Epoch: [1]  [ 72/540]  eta: 0:21:32  lr: 0.001000  obj_class_error: 100.00  loss: 11185.4893 (11282.8915)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.2638)  loss_sub_bbox: 5221.0083 (5484.2364)  loss_obj_bbox: 5767.8574 (5735.1079)  loss_sub_giou: 2.1737 (2.1966)  loss_obj_giou: 2.5764 (2.5747)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.7984)  loss_verb_ce_unscaled: 54.3694 (54.2638)  loss_sub_bbox_unscaled: 1044.2017 (1096.8473)  loss_obj_bbox_unscaled: 1153.5715 (1147.0216)  loss_sub_giou_unscaled: 1.0869 (1.0983)  loss_obj_giou_unscaled: 1.2882 (1.2873)  obj_cardinality_error_unscaled: 46.1000 (46.1260)  time: 2.6677  data: 1.8092  max mem: 2876\n",
      "Epoch: [1]  [ 73/540]  eta: 0:21:28  lr: 0.001000  obj_class_error: 100.00  loss: 11185.4893 (11279.5771)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 52.1060 (54.2346)  loss_sub_bbox: 5200.9204 (5480.1995)  loss_obj_bbox: 5790.8320 (5735.8609)  loss_sub_giou: 2.1737 (2.1955)  loss_obj_giou: 2.5764 (2.5744)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8011)  loss_verb_ce_unscaled: 52.1060 (54.2346)  loss_sub_bbox_unscaled: 1040.1841 (1096.0399)  loss_obj_bbox_unscaled: 1158.1664 (1147.1722)  loss_sub_giou_unscaled: 1.0869 (1.0977)  loss_obj_giou_unscaled: 1.2882 (1.2872)  obj_cardinality_error_unscaled: 46.1000 (46.1243)  time: 2.6779  data: 1.8176  max mem: 2876\n",
      "Epoch: [1]  [ 74/540]  eta: 0:21:23  lr: 0.001000  obj_class_error: 100.00  loss: 11185.4893 (11269.1601)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.2692)  loss_sub_bbox: 5185.5073 (5473.8979)  loss_obj_bbox: 5790.8320 (5731.7116)  loss_sub_giou: 2.1737 (2.1962)  loss_obj_giou: 2.5764 (2.5730)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8038)  loss_verb_ce_unscaled: 54.3694 (54.2692)  loss_sub_bbox_unscaled: 1037.1014 (1094.7796)  loss_obj_bbox_unscaled: 1158.1664 (1146.3423)  loss_sub_giou_unscaled: 1.0869 (1.0981)  loss_obj_giou_unscaled: 1.2882 (1.2865)  obj_cardinality_error_unscaled: 46.1000 (46.1307)  time: 2.6791  data: 1.8193  max mem: 2876\n",
      "Epoch: [1]  [ 75/540]  eta: 0:21:19  lr: 0.001000  obj_class_error: 100.00  loss: 11185.4893 (11274.0940)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.0859)  loss_sub_bbox: 5185.5073 (5474.8046)  loss_obj_bbox: 5790.8320 (5735.9200)  loss_sub_giou: 2.1737 (2.1967)  loss_obj_giou: 2.5764 (2.5745)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8064)  loss_verb_ce_unscaled: 54.3694 (54.0859)  loss_sub_bbox_unscaled: 1037.1014 (1094.9609)  loss_obj_bbox_unscaled: 1158.1664 (1147.1840)  loss_sub_giou_unscaled: 1.0869 (1.0983)  loss_obj_giou_unscaled: 1.2882 (1.2873)  obj_cardinality_error_unscaled: 46.1000 (46.1171)  time: 2.6693  data: 1.8122  max mem: 2876\n",
      "Epoch: [1]  [ 76/540]  eta: 0:21:15  lr: 0.001000  obj_class_error: 100.00  loss: 11185.4893 (11271.4689)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.4665)  loss_sub_bbox: 5185.5073 (5477.6765)  loss_obj_bbox: 5790.8320 (5730.0465)  loss_sub_giou: 2.1737 (2.1956)  loss_obj_giou: 2.5764 (2.5716)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8089)  loss_verb_ce_unscaled: 54.3694 (54.4665)  loss_sub_bbox_unscaled: 1037.1014 (1095.5353)  loss_obj_bbox_unscaled: 1158.1664 (1146.0093)  loss_sub_giou_unscaled: 1.0869 (1.0978)  loss_obj_giou_unscaled: 1.2882 (1.2858)  obj_cardinality_error_unscaled: 46.1000 (46.1299)  time: 2.6638  data: 1.8128  max mem: 2876\n",
      "Epoch: [1]  [ 77/540]  eta: 0:21:12  lr: 0.001000  obj_class_error: 100.00  loss: 11185.4893 (11278.8186)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.4362)  loss_sub_bbox: 5185.5073 (5478.2829)  loss_obj_bbox: 5790.8320 (5736.8215)  loss_sub_giou: 2.1657 (2.1941)  loss_obj_giou: 2.5764 (2.5718)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8113)  loss_verb_ce_unscaled: 54.3694 (54.4362)  loss_sub_bbox_unscaled: 1037.1014 (1095.6566)  loss_obj_bbox_unscaled: 1158.1664 (1147.3643)  loss_sub_giou_unscaled: 1.0828 (1.0970)  loss_obj_giou_unscaled: 1.2882 (1.2859)  obj_cardinality_error_unscaled: 46.1000 (46.1256)  time: 2.6652  data: 1.8126  max mem: 2876\n",
      "Epoch: [1]  [ 78/540]  eta: 0:21:07  lr: 0.001000  obj_class_error: 100.00  loss: 11242.7910 (11288.8237)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.5592)  loss_sub_bbox: 5185.5073 (5482.3465)  loss_obj_bbox: 5879.7319 (5742.6390)  loss_sub_giou: 2.1737 (2.1945)  loss_obj_giou: 2.5845 (2.5722)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8137)  loss_verb_ce_unscaled: 54.3694 (54.5592)  loss_sub_bbox_unscaled: 1037.1014 (1096.4693)  loss_obj_bbox_unscaled: 1175.9464 (1148.5278)  loss_sub_giou_unscaled: 1.0869 (1.0972)  loss_obj_giou_unscaled: 1.2922 (1.2861)  obj_cardinality_error_unscaled: 46.1000 (46.1329)  time: 2.6505  data: 1.7946  max mem: 2876\n",
      "Epoch: [1]  [ 79/540]  eta: 0:21:04  lr: 0.001000  obj_class_error: 100.00  loss: 11242.7910 (11277.6408)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 54.3694 (54.4257)  loss_sub_bbox: 5185.5073 (5478.0888)  loss_obj_bbox: 5879.7319 (5735.8455)  loss_sub_giou: 2.1788 (2.1960)  loss_obj_giou: 2.5845 (2.5726)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8161)  loss_verb_ce_unscaled: 54.3694 (54.4257)  loss_sub_bbox_unscaled: 1037.1014 (1095.6178)  loss_obj_bbox_unscaled: 1175.9464 (1147.1691)  loss_sub_giou_unscaled: 1.0894 (1.0980)  loss_obj_giou_unscaled: 1.2922 (1.2863)  obj_cardinality_error_unscaled: 46.1000 (46.1238)  time: 2.6514  data: 1.7886  max mem: 2876\n",
      "Epoch: [1]  [ 80/540]  eta: 0:21:00  lr: 0.001000  obj_class_error: 100.00  loss: 11185.4893 (11269.3125)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 52.1058 (54.2146)  loss_sub_bbox: 5185.5073 (5478.1024)  loss_obj_bbox: 5790.8320 (5727.7131)  loss_sub_giou: 2.1788 (2.1979)  loss_obj_giou: 2.5764 (2.5723)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8183)  loss_verb_ce_unscaled: 52.1058 (54.2146)  loss_sub_bbox_unscaled: 1037.1014 (1095.6205)  loss_obj_bbox_unscaled: 1158.1664 (1145.5426)  loss_sub_giou_unscaled: 1.0894 (1.0989)  loss_obj_giou_unscaled: 1.2882 (1.2862)  obj_cardinality_error_unscaled: 46.0000 (46.1074)  time: 2.6489  data: 1.7771  max mem: 2876\n",
      "Epoch: [1]  [ 81/540]  eta: 0:20:56  lr: 0.001000  obj_class_error: 100.00  loss: 11242.7910 (11282.3014)  loss_obj_ce: 4.5124 (4.5124)  loss_verb_ce: 52.1016 (53.9830)  loss_sub_bbox: 5200.9204 (5482.0628)  loss_obj_bbox: 5879.7319 (5736.9730)  loss_sub_giou: 2.2125 (2.1986)  loss_obj_giou: 2.5572 (2.5717)  loss_obj_ce_unscaled: 4.5124 (4.5124)  obj_class_error_unscaled: 100.0000 (99.8205)  loss_verb_ce_unscaled: 52.1016 (53.9830)  loss_sub_bbox_unscaled: 1040.1841 (1096.4126)  loss_obj_bbox_unscaled: 1175.9464 (1147.3946)  loss_sub_giou_unscaled: 1.1063 (1.0993)  loss_obj_giou_unscaled: 1.2786 (1.2859)  obj_cardinality_error_unscaled: 45.9000 (46.0963)  time: 2.6240  data: 1.7456  max mem: 2876\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 10\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn= collate_fn)\n",
    "\n",
    "import SSRT\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "reload(SSRT)\n",
    "model = SSRT.SSRT(num_oas, batch_size=batch_size, nobj = 91, nact =29, device = device )\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "train_one_epoch(model=model,criterion=criterion, optimizer=optimizer, data_loader=loader,device=device, epoch=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
