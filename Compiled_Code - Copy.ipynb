{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'OA_Candidate_Sampler' from 'c:\\\\Users\\\\bparaju\\\\OneDrive - Oklahoma A and M System\\\\ML Project\\\\Bibek\\\\Project Code\\\\OA_Candidate_Sampler.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Backbone\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from position_encoder import PositionEncoder\n",
    "from Encoder import Encoder\n",
    "import OA_Candidate_Sampler \n",
    "import torch.nn as nn\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from importlib import reload\n",
    "import json\n",
    "\n",
    "reload(Backbone)\n",
    "reload(OA_Candidate_Sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_OA_Pairs.json','r') as f:\n",
    "    UniqeOAPairs = json.load(f)\n",
    "\n",
    "UniqeOAPairs = [tuple(pair) for pair in UniqeOAPairs]\n",
    "num_oas = len(UniqeOAPairs) \n",
    "\n",
    "with open('OA_Pairs.json','r') as f:\n",
    "    id2OAPairs = json.load(f)\n",
    "\n",
    "id2OAPairs = {id: list(set(tuple(pair) for pair in id2OAPairs[id])) for id in id2OAPairs.keys()}\n",
    "\n",
    "with open('vcoco_processed_data_all_2014.json', 'r') as f:\n",
    "    vcoco_data = json.load(f)\n",
    "    \n",
    "ids = list(vcoco_data.keys())\n",
    "\n",
    "train_ids = ids[:8000]\n",
    "test_ids = ids[8000:10000]\n",
    "\n",
    "train_data = [vcoco_data[id] for id in train_ids]\n",
    "test_data = [vcoco_data[id] for id in test_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "id2OAOneHot = {}\n",
    "\n",
    "for id, pairs in id2OAPairs.items():\n",
    "    # Filter pairs that are in UniqeOAPairs\n",
    "    valid_pairs = [pair for pair in pairs if pair in UniqeOAPairs]\n",
    "\n",
    "    # Get indices of valid pairs\n",
    "    indices = [UniqeOAPairs.index(pair) for pair in valid_pairs]\n",
    "\n",
    "    # Convert indices to one-hot encoding\n",
    "    oneHot = nn.functional.one_hot(torch.tensor(indices, dtype=int), num_classes=len(UniqeOAPairs))#gives batch of hot encoding (len(indices), len(UNiqeOAPairs))\n",
    "    \n",
    "    oneHot = oneHot.sum(axis = 0) #converts to single vector (1, len(UNiqeOAPairs))\n",
    "    # Store the one-hot encodings for the ID\n",
    "    id2OAOneHot[id] = oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_image(file,url = False):\n",
    "    if(url):\n",
    "      response = requests.get(img_url)\n",
    "      img = Image.open(BytesIO(response.content))\n",
    "      img = img.convert(\"RGB\") #remove alpha channel\n",
    "    else:\n",
    "        folder = '../images/'\n",
    "        with open(folder+file, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            img = img.convert('RGB')\n",
    "    return img\n",
    "def get_batch(size=1):\n",
    "    images = []\n",
    "    y = []\n",
    "    idx = np.random.randint(0,len(train_data)-size)\n",
    "    for i,image in enumerate(train_data[idx:idx+size]):\n",
    "        img_url = image['url']\n",
    "        # img = get_image(img_url, url=True)\n",
    "        img = get_image(image['file_name'])\n",
    "        images.append(img)\n",
    "        try:\n",
    "            output = id2OAOneHot[str(image['id'])]\n",
    "        # print(image['id'],output)\n",
    "        except:\n",
    "            output = torch.zeros(len(UniqeOAPairs)) #if OA pair doesn't exist\n",
    "        y.append(output)\n",
    "        \n",
    "    return images,np.array(y)\n",
    "\n",
    "get_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ims, y = get_batch(20)\n",
    "\n",
    "feature_map = Backbone.Backbone()(ims) #output from backbone\n",
    "position_encoded_feature_map = PositionEncoder()(feature_map)\n",
    "B,C,W,H = position_encoded_feature_map.shape # (Batch, Channels, Width, Height)\n",
    "backbone_output_vectorized = position_encoded_feature_map.reshape((B,C,W*H)) #changing 2d image to vector (B, C, F) (F = feature)\n",
    "backbone_output_tansformed = backbone_output_vectorized.permute([0,2,1]) # (B, F, C)\n",
    "# backbone_output_tansformed = position_encoded_feature_map.permute([0,3,2,1]) # (B, H,W, C)\n",
    "encoder_output = Encoder()(backbone_output_tansformed) # (B, F, C)\n",
    "\n",
    "OASampler = OA_Candidate_Sampler.OACandidateSampler(num_oas)\n",
    "probabilities = OASampler(encoder_output)\n",
    "\n",
    "print(probabilities.shape)\n",
    "# confidences, indices = torch.topk(probabilities, 5, dim=-1)\n",
    "# print(\"Top-K OA Candidates (indices):\", indices)\n",
    "# print(\"Top-K OA Candidates (confidences):\", confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HOI_Decoder, HOI_Prediction, SpatialFeatureGenerator, Semantic_Feature_Generator, Feature_Aggregator, Query_Refiner, HOI_Decoder, HOI_Prediction\n",
    "\n",
    "reload(Query_Refiner)\n",
    "reload(HOI_Prediction)\n",
    "backbone = Backbone.Backbone()\n",
    "positionEncoder = PositionEncoder()\n",
    "matrix2Vector = VectorizeFeatureMap()\n",
    "encoder = Encoder()\n",
    "OASampler = OA_Candidate_Sampler.OACandidateSampler(num_oas)\n",
    "spatial_feature_generator = SpatialFeatureGenerator.SpatialFeatureGenerator([])\n",
    "semantic_generator = Semantic_Feature_Generator.SemanticFeatureGenerator()\n",
    "aggregator = Feature_Aggregator.FeatureAggregator()\n",
    "query_refiner = Query_Refiner.QueryRefiner(batch_size)\n",
    "decoder = HOI_Decoder.HOIDecoder()\n",
    "hoi_predictor = HOI_Prediction.HOIPrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "imgs, y = get_batch(batch_size)\n",
    "\n",
    "# Step 1: Initialize and call Backbone\n",
    "features = Backbone.Backbone()(imgs)\n",
    "\n",
    "# Step 2: Pass Backbone output to PositionEncoder\n",
    "position_encoded = PositionEncoder()(features)\n",
    "\n",
    "# Step 3: Pass PositionEncoder output to VectorizeFeatureMap\n",
    "vectorized_features = VectorizeFeatureMap()(position_encoded)\n",
    "\n",
    "# Step 4: Pass VectorizeFeatureMap output to Encoder\n",
    "encoded_features = Encoder()(vectorized_features)\n",
    "\n",
    "# Step 5: Pass Encoder output to OA_Candidate_Sampler\n",
    "oas = OA_Candidate_Sampler.OACandidateSampler(num_oas)(encoded_features)\n",
    "\n",
    "# Step 6: Pass OA_Candidate_Sampler output to SpatialFeatureGenerator\n",
    "spatial_features = SpatialFeatureGenerator.SpatialFeatureGenerator([])(oas)\n",
    "\n",
    "# Step 7: Pass SpatialFeatureGenerator output to SemanticFeatureGenerator\n",
    "semantic_features = Semantic_Feature_Generator.SemanticFeatureGenerator()(oas)\n",
    "\n",
    "# Step 8: Pass both SpatialFeatureGenerator and SemanticFeatureGenerator outputs to FeatureAggregator\n",
    "aggregated_features = Feature_Aggregator.FeatureAggregator()(spatial_features, semantic_features)\n",
    "\n",
    "# Step 9: Pass FeatureAggregator output to QueryRefiner\n",
    "refined_query = Query_Refiner.QueryRefiner(batch_size)(aggregated_features)\n",
    "\n",
    "# Step 10: Pass QueryRefiner output to HOI_Decoder\n",
    "decoded_query = HOI_Decoder.HOIDecoder()(encoded_features, refined_query)\n",
    "\n",
    "# Step 11: Pass HOI_Decoder output to HOI_Prediction\n",
    "bh, bo, po, pa = HOI_Prediction.HOIPrediction()(decoded_query)\n",
    "\n",
    "# Final Output\n",
    "# print(\"Human-Object Interaction Predictions:\", hoi_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Datasets\n",
    "import hico_dataset\n",
    "\n",
    "reload(Datasets)\n",
    "reload(hico_dataset)\n",
    "\n",
    "# dataset = Datasets.VCOCODataset(img_set=\"train\", img_folder=\"../images/\",anno_file=\"annotations/trainval_vcoco.json\" , num_queries=49, transforms=None)\n",
    "dataset = hico_dataset.HICODetection(img_set=\"train\", img_folder=\"../../HOTR/hico_20160224_det/images/train2015/\",anno_file=\"annotations/trainval_hico.json\" , num_queries=49, transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, DistributedSampler\n",
    "# import Set_Criterion\n",
    "# import Matcher\n",
    "# reload(Set_Criterion)\n",
    "\n",
    "# batch_size = 10\n",
    "\n",
    "# loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn= collate_fn)\n",
    "\n",
    "# losses = ['obj_labels', 'verb_labels', 'sub_obj_boxes', 'obj_cardinality']\n",
    "\n",
    "\n",
    "# weight_dict = {}\n",
    "# weight_dict['loss_obj_ce'] = 1\n",
    "# weight_dict['loss_verb_ce'] = 1\n",
    "# weight_dict['loss_sub_bbox'] = 5\n",
    "# weight_dict['loss_obj_bbox'] = 5\n",
    "# weight_dict['loss_sub_giou'] = 2\n",
    "# weight_dict['loss_obj_giou'] = 2\n",
    "\n",
    "# criterion= Set_Criterion.SetCriterionHOI(num_obj_classes=20, \n",
    "#                                          num_queries=49, \n",
    "#                                          num_verb_classes=29, \n",
    "#                                          matcher=Matcher.HungarianMatcherHOI(), \n",
    "#                                          weight_dict=weight_dict, \n",
    "#                                          eos_coef= 0.1 , \n",
    "#                                          losses= losses, verb_loss_type= 'focal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Set_Criterion\n",
    "import Matcher\n",
    "reload(Set_Criterion)\n",
    "losses = ['obj_labels', 'verb_labels', 'sub_obj_boxes', 'obj_cardinality']\n",
    "\n",
    "# weight_dict = model.state_dict()\n",
    "\n",
    "weight_dict = {}\n",
    "weight_dict['loss_obj_ce'] = 1\n",
    "weight_dict['loss_verb_ce'] = 1\n",
    "weight_dict['loss_sub_bbox'] = 5\n",
    "weight_dict['loss_obj_bbox'] = 5\n",
    "weight_dict['loss_sub_giou'] = 2\n",
    "weight_dict['loss_obj_giou'] = 2\n",
    "\n",
    "verbs_coco = 29\n",
    "n_objs_coco = 90\n",
    "\n",
    "verbs_hico = 117\n",
    "n_objs_hico = 90\n",
    "\n",
    "criterion= Set_Criterion.SetCriterionHOI(num_obj_classes=n_objs_hico, \n",
    "                                         num_queries=49, \n",
    "                                         num_verb_classes=verbs_hico, \n",
    "                                         matcher=Matcher.HungarianMatcherHOI(), \n",
    "                                         weight_dict=weight_dict, \n",
    "                                         eos_coef= 0.1 , \n",
    "                                         losses= losses, verb_loss_type= 'focal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = None\n",
    "# # def train_one_epoch():\n",
    "# #     model.train()\n",
    "# #     criterion.train()\n",
    "\n",
    "# batch_size = 10\n",
    "# loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn= collate_fn)\n",
    "\n",
    "# import SSRT\n",
    "\n",
    "# reload(SSRT)\n",
    "# model = SSRT.SSRT(num_oas, batch_size=batch_size, nobj = 91, nact =29)\n",
    "\n",
    "\n",
    "# for images, targets in loader:\n",
    "#     # images = images.to(device)\n",
    "#     # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#     outputs = model(list(images))\n",
    "#     loss_dict = criterion(outputs, targets)\n",
    "#     weight_dict = criterion.weight_dict\n",
    "#     losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "#     # reduce losses over all GPUs for logging purposes\n",
    "#     # loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "#     loss_dict_reduced = loss_dict\n",
    "#     loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "#                                     for k, v in loss_dict_reduced.items()}\n",
    "#     loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "#                                 for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "#     losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "#     # loss_value = losses_reduced_scaled.item()\n",
    "#     losses.backward()\n",
    "\n",
    "#     # return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Copyright (c) Hitachi, Ltd. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n",
    "# ------------------------------------------------------------------------\n",
    "# Modified from DETR (https://github.com/facebookresearch/detr)\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "# ------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "Train and eval functions used in main.py\n",
    "\"\"\"\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from typing import Iterable\n",
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "import misc \n",
    "\n",
    "reload(misc)\n",
    "\n",
    "\n",
    "utils = misc\n",
    "\n",
    "\n",
    "def train_one_epoch(model, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    if hasattr(criterion, 'loss_labels'):\n",
    "        metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    else:\n",
    "        metric_logger.add_meter('obj_class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 1\n",
    "\n",
    "    \n",
    "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        # samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # targets = torch.stack(targets, dim = 0).to(device)\n",
    "        # samples = list(samples)\n",
    "       \n",
    "\n",
    "        # Apply transformation to each image and convert it to tensor\n",
    "        # samples = [transform(image) for image in samples]\n",
    "        # samples = torch.stack(samples, dim=0)\n",
    "\n",
    "        outputs = model(samples)\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "        loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if max_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "        if hasattr(criterion, 'loss_labels'):\n",
    "            metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "        else:\n",
    "            metric_logger.update(obj_class_error=loss_dict_reduced['obj_class_error'])\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [  0/100]  eta: 0:02:13  lr: 0.010000  obj_class_error: 100.00  loss: 11513.6934 (11513.6934)  loss_obj_ce: 3.6745 (3.6745)  loss_verb_ce: 116.8769 (116.8769)  loss_sub_bbox: 6006.5488 (6006.5488)  loss_obj_bbox: 5381.6826 (5381.6826)  loss_sub_giou: 2.8022 (2.8022)  loss_obj_giou: 2.1083 (2.1083)  loss_obj_ce_unscaled: 3.6745 (3.6745)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 116.8769 (116.8769)  loss_sub_bbox_unscaled: 1201.3098 (1201.3098)  loss_obj_bbox_unscaled: 1076.3365 (1076.3365)  loss_sub_giou_unscaled: 1.4011 (1.4011)  loss_obj_giou_unscaled: 1.0541 (1.0541)  obj_cardinality_error_unscaled: 7.1000 (7.1000)  time: 1.3308  data: 0.0319  max mem: 29070\n",
      "Epoch: [1]  [  1/100]  eta: 0:02:02  lr: 0.010000  obj_class_error: 100.00  loss: 11473.6367 (11493.6650)  loss_obj_ce: 3.5928 (3.6336)  loss_verb_ce: 116.8769 (189.1560)  loss_sub_bbox: 5361.7744 (5684.1616)  loss_obj_bbox: 5381.6826 (5611.9609)  loss_sub_giou: 2.1338 (2.4680)  loss_obj_giou: 2.1083 (2.2853)  loss_obj_ce_unscaled: 3.5928 (3.6336)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 116.8769 (189.1560)  loss_sub_bbox_unscaled: 1072.3549 (1136.8323)  loss_obj_bbox_unscaled: 1076.3365 (1122.3922)  loss_sub_giou_unscaled: 1.0669 (1.2340)  loss_obj_giou_unscaled: 1.0541 (1.1427)  obj_cardinality_error_unscaled: 3.1000 (5.1000)  time: 1.2330  data: 0.0255  max mem: 29070\n",
      "Epoch: [1]  [  2/100]  eta: 0:01:54  lr: 0.010000  obj_class_error: 100.00  loss: 11473.6367 (11329.9144)  loss_obj_ce: 3.5928 (3.6071)  loss_verb_ce: 261.4352 (333.0735)  loss_sub_bbox: 5361.7744 (5489.7189)  loss_obj_bbox: 5381.6826 (5498.7926)  loss_sub_giou: 2.3099 (2.4153)  loss_obj_giou: 2.3520 (2.3075)  loss_obj_ce_unscaled: 3.5928 (3.6071)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 261.4352 (333.0735)  loss_sub_bbox_unscaled: 1072.3549 (1097.9438)  loss_obj_bbox_unscaled: 1076.3365 (1099.7585)  loss_sub_giou_unscaled: 1.1549 (1.2076)  loss_obj_giou_unscaled: 1.1760 (1.1538)  obj_cardinality_error_unscaled: 3.1000 (3.8000)  time: 1.1708  data: 0.0287  max mem: 29070\n",
      "Epoch: [1]  [  3/100]  eta: 0:01:53  lr: 0.010000  obj_class_error: 100.00  loss: 11473.6367 (11389.1797)  loss_obj_ce: 3.5663 (3.5969)  loss_verb_ce: 261.4352 (338.5063)  loss_sub_bbox: 5176.1113 (5411.3170)  loss_obj_bbox: 5381.6826 (5631.1001)  loss_sub_giou: 2.1338 (2.3385)  loss_obj_giou: 2.3520 (2.3213)  loss_obj_ce_unscaled: 3.5663 (3.5969)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 261.4352 (338.5063)  loss_sub_bbox_unscaled: 1035.2223 (1082.2634)  loss_obj_bbox_unscaled: 1076.3365 (1126.2200)  loss_sub_giou_unscaled: 1.0669 (1.1693)  loss_obj_giou_unscaled: 1.1760 (1.1606)  obj_cardinality_error_unscaled: 1.8000 (3.3000)  time: 1.1721  data: 0.0300  max mem: 29070\n",
      "Epoch: [1]  [  4/100]  eta: 0:03:32  lr: 0.010000  obj_class_error: 100.00  loss: 11473.6367 (11394.7699)  loss_obj_ce: 3.5724 (3.5920)  loss_verb_ce: 354.8048 (350.2814)  loss_sub_bbox: 5361.7744 (5453.5774)  loss_obj_bbox: 5389.0225 (5582.6846)  loss_sub_giou: 2.3099 (2.3355)  loss_obj_giou: 2.3520 (2.2994)  loss_obj_ce_unscaled: 3.5724 (3.5920)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 354.8048 (350.2814)  loss_sub_bbox_unscaled: 1072.3549 (1090.7155)  loss_obj_bbox_unscaled: 1077.8044 (1116.5369)  loss_sub_giou_unscaled: 1.1549 (1.1677)  loss_obj_giou_unscaled: 1.1760 (1.1497)  obj_cardinality_error_unscaled: 2.1000 (3.0600)  time: 2.2091  data: 0.0280  max mem: 29070\n",
      "Epoch: [1]  [  5/100]  eta: 0:04:09  lr: 0.010000  obj_class_error: 100.00  loss: 11473.6367 (11475.1017)  loss_obj_ce: 3.5724 (3.5915)  loss_verb_ce: 275.9593 (337.8944)  loss_sub_bbox: 5361.7744 (5491.0846)  loss_obj_bbox: 5389.0225 (5637.8659)  loss_sub_giou: 2.3099 (2.3448)  loss_obj_giou: 2.3520 (2.3208)  loss_obj_ce_unscaled: 3.5724 (3.5915)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 275.9593 (337.8944)  loss_sub_bbox_unscaled: 1072.3549 (1098.2169)  loss_obj_bbox_unscaled: 1077.8044 (1127.5732)  loss_sub_giou_unscaled: 1.1549 (1.1724)  loss_obj_giou_unscaled: 1.1760 (1.1604)  obj_cardinality_error_unscaled: 2.1000 (3.0333)  time: 2.6233  data: 0.0268  max mem: 29070\n",
      "Epoch: [1]  [  6/100]  eta: 0:03:46  lr: 0.010000  obj_class_error: 100.00  loss: 11513.6934 (11794.1455)  loss_obj_ce: 3.5724 (3.5873)  loss_verb_ce: 354.8048 (354.1337)  loss_sub_bbox: 5622.6191 (5591.8672)  loss_obj_bbox: 5842.2393 (5839.8609)  loss_sub_giou: 2.3233 (2.3433)  loss_obj_giou: 2.3626 (2.3534)  loss_obj_ce_unscaled: 3.5724 (3.5873)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 354.8048 (354.1337)  loss_sub_bbox_unscaled: 1124.5238 (1118.3734)  loss_obj_bbox_unscaled: 1168.4479 (1167.9722)  loss_sub_giou_unscaled: 1.1617 (1.1716)  loss_obj_giou_unscaled: 1.1813 (1.1767)  obj_cardinality_error_unscaled: 2.1000 (2.8286)  time: 2.4045  data: 0.0259  max mem: 29070\n",
      "Epoch: [1]  [  7/100]  eta: 0:03:28  lr: 0.010000  obj_class_error: 100.00  loss: 11513.6934 (11807.7645)  loss_obj_ce: 3.5663 (3.5847)  loss_verb_ce: 354.8048 (363.8590)  loss_sub_bbox: 5622.6191 (5625.5227)  loss_obj_bbox: 5601.9136 (5810.1175)  loss_sub_giou: 2.3233 (2.3565)  loss_obj_giou: 2.3520 (2.3243)  loss_obj_ce_unscaled: 3.5663 (3.5847)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 354.8048 (363.8590)  loss_sub_bbox_unscaled: 1124.5238 (1125.1045)  loss_obj_bbox_unscaled: 1120.3827 (1162.0235)  loss_sub_giou_unscaled: 1.1617 (1.1782)  loss_obj_giou_unscaled: 1.1760 (1.1622)  obj_cardinality_error_unscaled: 1.8000 (2.7000)  time: 2.2371  data: 0.0248  max mem: 29070\n",
      "Epoch: [1]  [  8/100]  eta: 0:04:14  lr: 0.010000  obj_class_error: 100.00  loss: 11513.6934 (11617.0023)  loss_obj_ce: 3.5724 (3.5881)  loss_verb_ce: 354.8048 (346.9162)  loss_sub_bbox: 5622.6191 (5488.9302)  loss_obj_bbox: 5601.9136 (5772.8676)  loss_sub_giou: 2.3340 (2.3890)  loss_obj_giou: 2.3520 (2.3114)  loss_obj_ce_unscaled: 3.5724 (3.5881)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 354.8048 (346.9162)  loss_sub_bbox_unscaled: 1124.5238 (1097.7861)  loss_obj_bbox_unscaled: 1120.3827 (1154.5735)  loss_sub_giou_unscaled: 1.1670 (1.1945)  loss_obj_giou_unscaled: 1.1760 (1.1557)  obj_cardinality_error_unscaled: 2.1000 (2.8667)  time: 2.7680  data: 0.0240  max mem: 29070\n",
      "Epoch: [1]  [  9/100]  eta: 0:04:07  lr: 0.010000  obj_class_error: 100.00  loss: 11473.6367 (11536.6552)  loss_obj_ce: 3.5724 (3.5871)  loss_verb_ce: 292.1922 (341.4438)  loss_sub_bbox: 5622.6191 (5502.8497)  loss_obj_bbox: 5474.8682 (5684.0788)  loss_sub_giou: 2.3340 (2.4022)  loss_obj_giou: 2.2117 (2.2937)  loss_obj_ce_unscaled: 3.5724 (3.5871)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 292.1922 (341.4438)  loss_sub_bbox_unscaled: 1124.5238 (1100.5699)  loss_obj_bbox_unscaled: 1094.9736 (1136.8158)  loss_sub_giou_unscaled: 1.1670 (1.2011)  loss_obj_giou_unscaled: 1.1059 (1.1468)  obj_cardinality_error_unscaled: 2.1000 (2.8200)  time: 2.7214  data: 0.0236  max mem: 29070\n",
      "Epoch: [1]  [ 10/100]  eta: 0:03:51  lr: 0.010000  obj_class_error: 100.00  loss: 11513.6934 (11537.4994)  loss_obj_ce: 3.5724 (3.5847)  loss_verb_ce: 354.8048 (353.4101)  loss_sub_bbox: 5622.6191 (5468.7119)  loss_obj_bbox: 5601.9136 (5707.1286)  loss_sub_giou: 2.3340 (2.3830)  loss_obj_giou: 2.2117 (2.2812)  loss_obj_ce_unscaled: 3.5724 (3.5847)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 354.8048 (353.4101)  loss_sub_bbox_unscaled: 1124.5238 (1093.7424)  loss_obj_bbox_unscaled: 1120.3827 (1141.4257)  loss_sub_giou_unscaled: 1.1670 (1.1915)  loss_obj_giou_unscaled: 1.1059 (1.1406)  obj_cardinality_error_unscaled: 2.1000 (2.7000)  time: 2.5711  data: 0.0229  max mem: 29070\n",
      "Epoch: [1]  [ 11/100]  eta: 0:03:37  lr: 0.010000  obj_class_error: 100.00  loss: 11473.6367 (11442.1963)  loss_obj_ce: 3.5724 (3.5838)  loss_verb_ce: 342.5702 (352.5067)  loss_sub_bbox: 5361.7744 (5405.9783)  loss_obj_bbox: 5474.8682 (5675.4909)  loss_sub_giou: 2.3233 (2.3603)  loss_obj_giou: 2.2117 (2.2762)  loss_obj_ce_unscaled: 3.5724 (3.5838)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 342.5702 (352.5067)  loss_sub_bbox_unscaled: 1072.3549 (1081.1957)  loss_obj_bbox_unscaled: 1094.9736 (1135.0982)  loss_sub_giou_unscaled: 1.1617 (1.1802)  loss_obj_giou_unscaled: 1.1059 (1.1381)  obj_cardinality_error_unscaled: 2.1000 (2.6583)  time: 2.4386  data: 0.0222  max mem: 29070\n",
      "Epoch: [1]  [ 12/100]  eta: 0:03:25  lr: 0.010000  obj_class_error: 100.00  loss: 11473.6367 (11386.6091)  loss_obj_ce: 3.5745 (3.5855)  loss_verb_ce: 342.5702 (342.7589)  loss_sub_bbox: 5361.7744 (5365.8823)  loss_obj_bbox: 5600.6602 (5669.7347)  loss_sub_giou: 2.3340 (2.3843)  loss_obj_giou: 2.2117 (2.2635)  loss_obj_ce_unscaled: 3.5745 (3.5855)  obj_class_error_unscaled: 100.0000 (100.0000)  loss_verb_ce_unscaled: 342.5702 (342.7589)  loss_sub_bbox_unscaled: 1072.3549 (1073.1765)  loss_obj_bbox_unscaled: 1120.1321 (1133.9469)  loss_sub_giou_unscaled: 1.1670 (1.1921)  loss_obj_giou_unscaled: 1.1059 (1.1317)  obj_cardinality_error_unscaled: 2.2000 (2.7385)  time: 2.3322  data: 0.0217  max mem: 29070\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from torch.utils.data.dataloader import DataLoader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# def collate_fn(batch):\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     batch = list(zip(*batch))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# model.to(device)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[0;32m     46\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# targets = torch.stack(targets, dim = 0).to(device)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# samples = list(samples)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# samples = [transform(image) for image in samples]\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# samples = torch.stack(samples, dim=0)\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     57\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mweight_dict\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\OneDrive - Oklahoma A and M System\\ML Project\\Bibek\\Project Code\\SSRT.py:60\u001b[0m, in \u001b[0;36mSSRT.forward\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m     57\u001b[0m spatial_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_feature_generator(oas)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Step 7: Semantic Feature Generator\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m semantic_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msemantic_feature_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43moas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Step 8: Feature Aggregator\u001b[39;00m\n\u001b[0;32m     63\u001b[0m aggregated_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_aggregator(spatial_features, semantic_features)\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bparaju\\OneDrive - Oklahoma A and M System\\ML Project\\Bibek\\Project Code\\Semantic_Feature_Generator.py:59\u001b[0m, in \u001b[0;36mSemanticFeatureGenerator.forward\u001b[1;34m(self, oa_ids)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[1;32m---> 59\u001b[0m         oa_id \u001b[38;5;241m=\u001b[39m \u001b[43moa_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fetch the ID\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# oa_candidate = id2OaCansdidate[oa_id]  # Map ID to OA candidate (action, object)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m         oa_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_oa_candidate(oa_id)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from torch.utils.data.dataloader import DataLoader\n",
    "# def collate_fn(batch):\n",
    "#     batch = list(zip(*batch))\n",
    "#     return tuple(batch)\n",
    "\n",
    "\n",
    "# batch_size = 10\n",
    "# loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn= collate_fn)\n",
    "\n",
    "# # nobj = n_objs_hico\n",
    "# nact = verbs_hico\n",
    "\n",
    "\n",
    "# nobj = 91\n",
    "# # nact = verbs_coco\n",
    "\n",
    "# import SSRT\n",
    "# reload(SSRT)\n",
    "# device = torch.device(\"cuda\")\n",
    "# OA_distn = load_OA_distributions(device)\n",
    "# num_oas = len(OA_distn.keys())\n",
    "# model = SSRT.SSRT(num_oas, OA_distn = OA_distn, batch_size=batch_size, nobj = nobj, nact =nact, device = device, n_queries = 49 )\n",
    "# model.to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "train_one_epoch(model=model,criterion=criterion, optimizer=optimizer, data_loader=loader,device=device, epoch=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def replace_nan_with_zero(arr):\n",
    "    \"\"\"\n",
    "    Replaces any 'nan' values in a list with 0 (handles both 1D and 2D arrays).\n",
    "    \"\"\"\n",
    "    if isinstance(arr, list):\n",
    "        return [[0 if isinstance(i, float) and np.isnan(i) else i for i in sublist] for sublist in arr]\n",
    "    return arr\n",
    "\n",
    "def safely_evaluate(value):\n",
    "    \"\"\"\n",
    "    Safely evaluates a string, replacing 'nan' with float('nan').\n",
    "    \"\"\"\n",
    "    value = value.replace('nan', 'float(\"nan\")')  # Replace 'nan' with float('nan')\n",
    "    try:\n",
    "        return eval(value)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating value: {value} -> {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_OA_distributions(device):\n",
    "    data = {}\n",
    "\n",
    "    with open('unique_means.txt', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            # Split the line by semicolon\n",
    "            parts = line.strip().split(';')\n",
    "\n",
    "            # Extract and process each part\n",
    "            item_action = parts[0].strip()\n",
    "            word_list = item_action.strip(\"()\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "\n",
    "            # Convert list_1, array_1, list_2, array_2 into valid Python objects using safely_evaluate\n",
    "            list_1 = safely_evaluate(parts[1].strip())  # Convert string to list, e.g., [0.197, 0.297]\n",
    "            array_1 = safely_evaluate(parts[2].strip())  # Convert string to nested list, e.g., [[nan, nan], [nan, nan]]\n",
    "            list_2 = safely_evaluate(parts[3].strip())  # Convert string to list, e.g., [-1.82, -1.58]\n",
    "            array_2 = safely_evaluate(parts[4].strip())  # Convert string to nested list, e.g., [[nan, nan], [nan, nan]]\n",
    "\n",
    "            if list_1 is None or array_1 is None or list_2 is None or array_2 is None:\n",
    "                continue  # Skip this line if evaluation failed\n",
    "\n",
    "            # Replace 'nan' with 0 in array_1 and array_2\n",
    "            array_1 = replace_nan_with_zero(array_1)\n",
    "            array_2 = replace_nan_with_zero(array_2)\n",
    "            \n",
    "\n",
    "            small_value = 1e-3\n",
    "\n",
    "\n",
    "            data[i] = {\n",
    "                'OA_pair': word_list,\n",
    "                'mean_xy': torch.tensor(list_1, dtype=torch.float32).to(device)  if isinstance(list_1, list) and all(isinstance(i, (int, float)) for i in list_1) else None,\n",
    "                'cov_xy': (torch.tensor(array_1, dtype=torch.float32) + small_value * torch.eye(len(array_2), dtype=torch.float32)).to(device) if isinstance(array_1, list) and all(isinstance(i, list) and all(isinstance(j, (int, float)) for j in i) for i in array_1) else None,\n",
    "                'mean_wh': torch.tensor(list_2, dtype=torch.float32).to(device) if isinstance(list_2, list) and all(isinstance(i, (int, float)) for i in list_2) else None,\n",
    "                'cov_wh': (torch.tensor(array_2, dtype=torch.float32) + + small_value * torch.eye(len(array_2), dtype=torch.float32)).to(device) if isinstance(array_2, list) and all(isinstance(i, list) and all(isinstance(j, (int, float)) for j in i) for i in array_2) else None\n",
    "            }\n",
    "\n",
    "\n",
    "        return data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
